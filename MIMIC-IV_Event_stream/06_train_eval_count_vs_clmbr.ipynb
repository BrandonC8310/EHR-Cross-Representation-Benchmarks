{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Evaluate: COUNT vs CLMBR (mortality + phenotyping)\n\n## Purpose\nTrain logistic regression baselines on COUNT and CLMBR features for both tasks; includes both single-label and multi-label handling.\n\n## Inputs\n- COUNT and CLMBR feature PKLs from step 04\n- Split definitions / shot files (if used)\n\n## Outputs\n- Model metrics, predictions, and result artifacts written to your output directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c67bcc-81e2-4a06-90b6-a22fa01802f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, datetime, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, accuracy_score\n",
    "from loguru import logger\n",
    "\n",
    "from ehrshot.labelers.core import load_labeled_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d025f9f-03ed-4cd7-8b81-9189ed7dbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/root/autodl-tmp/femr\"\n",
    "LABELING_FUNCTION = \"mimic_icu_phenotyping\"\n",
    "\n",
    "TRAIN = {\n",
    "    \"db\":       f\"{ROOT}/train/extract\",\n",
    "    \"labels\":   f\"{ROOT}/train/femr_labels\",\n",
    "    \"features\": f\"{ROOT}/train/femr_features\",\n",
    "}\n",
    "TUNING = {\n",
    "    \"db\":       f\"{ROOT}/tuning/extract\",\n",
    "    \"labels\":   f\"{ROOT}/tuning/femr_labels\",\n",
    "    \"features\": f\"{ROOT}/tuning/femr_features\",\n",
    "}\n",
    "HELDOUT = {\n",
    "    \"db\":       f\"{ROOT}/held_out/extract\",\n",
    "    \"labels\":   f\"{ROOT}/held_out/femr_labels\",\n",
    "    \"features\": f\"{ROOT}/held_out/femr_features\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869f7fc-3092-4078-8d72-f47275a7948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOT_STRAT = \"all\"  # \"few\"\n",
    "SHOTS_JSON = os.path.join(TRAIN[\"labels\"], LABELING_FUNCTION, f\"{SHOT_STRAT}_shots_data.json\")\n",
    "K_VALUES   = [-1]  # all ：[-1]；few ：[8,16,32]\n",
    "MODELS = [\"count\", \"clmbr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2227665-154b-4157-a433-12c32de2b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_FILES = {\n",
    "    \"train\": os.path.join(TRAIN[\"features\"], LABELING_FUNCTION, \"count_features.pkl\"),\n",
    "    \"tuning\": os.path.join(TUNING[\"features\"],LABELING_FUNCTION, \"count_features.pkl\"),\n",
    "    \"held\":   os.path.join(HELDOUT[\"features\"], LABELING_FUNCTION,\"count_features.pkl\"),\n",
    "}\n",
    "\n",
    "CLMBR_FILES = {\n",
    "    \"train\": os.path.join(TRAIN[\"features\"],LABELING_FUNCTION, \"clmbr_features.pkl\"),\n",
    "    \"tuning\":os.path.join(TUNING[\"features\"],LABELING_FUNCTION, \"clmbr_features.pkl\"),\n",
    "    \"held\":  os.path.join(HELDOUT[\"features\"], LABELING_FUNCTION, \"clmbr_features.pkl\"),\n",
    "}\n",
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea47a2-aaec-4426-86ee-479ea2e50d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Count features loader ------------------\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "\n",
    "def _to_minute_iso(x):\n",
    "    dt = pd.to_datetime(x).to_pydatetime()\n",
    "    return dt.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "\n",
    "def load_count_split(path_pkl):\n",
    "    \"\"\"\n",
    "     count_features.pkl，：(X_csr, patient_ids, label_values, label_times)\n",
    "    : (X_csr, rows_df[patient_id,time], y[0/1])\n",
    "    \"\"\"\n",
    "    with open(path_pkl, \"rb\") as f:\n",
    "        X, pids, y, times = pickle.load(f)\n",
    "\n",
    "    if not issparse(X):\n",
    "        X = csr_matrix(np.asarray(X))\n",
    "    else:\n",
    "        X = X.tocsr()\n",
    "\n",
    "    rows_df = pd.DataFrame({\n",
    "        \"patient_id\": np.asarray(pids, dtype=int),\n",
    "        \"time\": [ _to_minute_iso(t) for t in times ],\n",
    "    })\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    if y.dtype == bool:\n",
    "        y = y.astype(int)\n",
    "    elif np.issubdtype(y.dtype, np.integer):\n",
    "        y = y.astype(int)\n",
    "    else:\n",
    "        try:\n",
    "            y = y.astype(int)\n",
    "        except Exception:\n",
    "            vals = pd.Series(y).map(lambda v: 1 if str(v).strip() in {\"1\",\"True\",\"true\"} else 0)\n",
    "            y = vals.to_numpy(dtype=int)\n",
    "\n",
    "    assert X.shape[0] == len(rows_df) == len(y), \\\n",
    "        f\": X={X.shape[0]}, rows={len(rows_df)}, y={len(y)}\"\n",
    "\n",
    "    return X, rows_df, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f58c5-ef7d-4cd0-bb89-724359510e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "COUNT_FILES = {\n",
    "    \"train\": f\"/root/autodl-tmp/femr/train/femr_features/{LABELING_FUNCTION}/count_features.pkl\",\n",
    "    \"tuning\":f\"/root/autodl-tmp/femr/tuning/femr_features/{LABELING_FUNCTION}/count_features.pkl\",\n",
    "    \"held\":  f\"/root/autodl-tmp/femr/held_out/femr_features/{LABELING_FUNCTION}/count_features.pkl\",\n",
    "}\n",
    "\n",
    "for split, p in COUNT_FILES.items():\n",
    "    X, rows, y = load_count_split(p)\n",
    "    pos = int(y.sum()); n = len(y)\n",
    "    print(f\"{split:7s} | X={X.shape}, rows={rows.shape}, pos={pos}/{n} ({pos/n:.3%})\")\n",
    "    print(rows.head(2), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48f28c-2cf5-4962-9175-8c1b49d39e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X, rows, y = load_count_split(COUNT_FILES[\"train\"])\n",
    "idxs = rng.choice(len(rows), size=3, replace=False)\n",
    "print(rows.iloc[idxs].assign(y=y[idxs]).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ee330-ff2e-476f-bcee-8dd435f82adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
    "\n",
    "def f1_at_threshold(y_true, prob, thr: float):\n",
    "    pred = (prob >= thr).astype(int)\n",
    "    return {\n",
    "        \"precision\": float(precision_score(y_true, pred, zero_division=0)),\n",
    "        \"recall\":    float(recall_score(y_true, pred, zero_division=0)),\n",
    "        \"f1\":        float(f1_score(y_true, pred, zero_division=0)),\n",
    "        \"thr\":       float(thr),\n",
    "    }\n",
    "\n",
    "def find_best_f1_threshold(y_true, prob):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, prob)\n",
    "    f1s = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "    i = int(f1s.argmax())\n",
    "    return float(thr[i]), float(f1s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f1a1e-34ec-470e-bd72-65c5221ea500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, datetime, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, accuracy_score\n",
    "from loguru import logger\n",
    "\n",
    "from ehrshot.labelers.core import load_labeled_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1fedd-511a-4daa-a52d-61fecafb3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shots(shots_json: str, task: str, ks: List[int]) -> Dict[int, Dict[int, dict]]:\n",
    "    \"\"\" {k: {replicate: payload}}； ks  k，。\"\"\"\n",
    "    with open(shots_json, \"r\") as f:\n",
    "        blob = json.load(f)\n",
    "    assert task in blob, f\"{shots_json}  {task}\"\n",
    "    task_dict = blob[task]\n",
    "    out: Dict[int, Dict[int, dict]] = {}\n",
    "    for k in ks:\n",
    "        if str(k) in task_dict:\n",
    "            out[k] = {int(rep): v for rep, v in task_dict[str(k)].items()}\n",
    "        elif k in task_dict:\n",
    "            out[k] = {int(rep): v for rep, v in task_dict[k].items()}\n",
    "        else:\n",
    "            logger.warning(f\"[shots] k={k}  {shots_json} ，\")\n",
    "    return out\n",
    "\n",
    "def instances_from_labeled_csv(path_to_labels_dir: str, task: str) -> List[Tuple[int, str, int]]:\n",
    "    \"\"\" labeled_patients.csv， (patient_id, time_iso, value0/1) \"\"\"\n",
    "    lp_csv = os.path.join(path_to_labels_dir, task, \"labeled_patients.csv\")\n",
    "    lp = load_labeled_patients(lp_csv)\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for pid, labels in lp.items():\n",
    "        for lab in labels:\n",
    "            t_iso = lab.time.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "            key = (int(pid), t_iso)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            val = 1 if bool(lab.value) else 0\n",
    "            out.append((int(pid), t_iso, val))\n",
    "    return out\n",
    "\n",
    "def _coerce_time_str(x: str) -> str:\n",
    "    \"\"\" ISO（YYYY-mm-ddTHH:MM）\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            dt = datetime.datetime.fromisoformat(x)\n",
    "        except Exception:\n",
    "            try:\n",
    "                dt = pd.to_datetime(x).to_pydatetime()\n",
    "            except Exception:\n",
    "                raise\n",
    "    else:\n",
    "        dt = x\n",
    "    return dt.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "\n",
    "def _load_rows_csv(rows_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(rows_path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    pid_col = cols.get(\"patient_id\") or cols.get(\"pid\") or list(df.columns)[0]\n",
    "    time_col = cols.get(\"time\") or cols.get(\"label_time\") or cols.get(\"timestamp\") or list(df.columns)[1]\n",
    "    df = df[[pid_col, time_col]].copy()\n",
    "    df.columns = [\"patient_id\", \"time\"]\n",
    "    df[\"patient_id\"] = df[\"patient_id\"].astype(int)\n",
    "    df[\"time\"] = df[\"time\"].apply(_coerce_time_str)\n",
    "    return df\n",
    "\n",
    "def _join_rows(rows_df: pd.DataFrame, samples: List[Tuple[int, str, int]]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" (row_indices, y)； rows_df ，。\"\"\"\n",
    "    key2row = {(int(pid), t): i for i, (pid, t) in enumerate(zip(rows_df[\"patient_id\"].values,\n",
    "                                                                 rows_df[\"time\"].values))}\n",
    "    idxs = []\n",
    "    ys = []\n",
    "    missed = 0\n",
    "    for pid, t_iso, y in samples:\n",
    "        r = key2row.get((pid, t_iso))\n",
    "        if r is None:\n",
    "            missed += 1\n",
    "            continue\n",
    "        idxs.append(r)\n",
    "        ys.append(int(y))\n",
    "    if missed > 0:\n",
    "        logger.warning(f\"[align]  {missed}  rows.csv ，\")\n",
    "    return np.array(idxs, dtype=int), np.array(ys, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8298bf5-3c52-487d-866f-23462387e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ CLMBR features loader ------------------\n",
    "import os, datetime\n",
    "import pickle, numpy as np, pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def _first_present(d: dict, keys):\n",
    "    \"\"\" (value, key)；， (None, None)。\"\"\"\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k], k\n",
    "    return None, None\n",
    "\n",
    "def load_clmbr_split(pkl_path: str) -> Tuple[object, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    ：\n",
    "    - dict: {\"features\"/\"representations\"/\"X\"/\"data_matrix\": array-like or sparse,\n",
    "             \"patient_id\"/\"patient_ids\"/\"pids\": array-like,\n",
    "             \"time\"/\"times\"/\"label_times\"/\"labeling_time\": array-like,\n",
    "             （）\"label_values\"/\"labels\"/\"y\": array-like}\n",
    "    - (features, rows_df-like) \n",
    "    \"\"\"\n",
    "    assert os.path.exists(pkl_path), f\" CLMBR : {pkl_path}\"\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        feats, fk   = _first_present(obj, [\"features\", \"representations\", \"X\", \"data_matrix\"])\n",
    "        pids, pk    = _first_present(obj, [\"patient_id\", \"patient_ids\", \"pids\"])\n",
    "        times, tk   = _first_present(obj, [\"time\", \"times\", \"label_times\", \"labeling_time\"])\n",
    "        labels, lk  = _first_present(obj, [\"label_values\", \"labels\", \"y\"])\n",
    "\n",
    "        if feats is None or pids is None or times is None:\n",
    "            raise ValueError(\n",
    "                f\"{pkl_path} ；：{list(obj.keys())}\\n\"\n",
    "                f\" features/representations/X/data_matrix + patient_id/patient_ids/pids + time/times/label_times/labeling_time\"\n",
    "            )\n",
    "\n",
    "        def _to_iso_minute(x):\n",
    "            if isinstance(x, str):\n",
    "                try:\n",
    "                    dt = datetime.datetime.fromisoformat(x)\n",
    "                except Exception:\n",
    "                    dt = pd.to_datetime(x).to_pydatetime()\n",
    "            else:\n",
    "                dt = pd.to_datetime(x).to_pydatetime()\n",
    "            return dt.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "\n",
    "        rows_df = pd.DataFrame({\n",
    "            \"patient_id\": np.asarray(pids).astype(int),\n",
    "            \"time\": [ _to_iso_minute(t) for t in times ],\n",
    "        })\n",
    "        if labels is not None and len(labels) == len(rows_df):\n",
    "            rows_df[\"label\"] = np.asarray(labels)\n",
    "        return feats, rows_df\n",
    "\n",
    "    if isinstance(obj, (list, tuple)) and len(obj) == 2:\n",
    "        feats, rows = obj\n",
    "        if isinstance(rows, pd.DataFrame):\n",
    "            rows_df = rows.copy()\n",
    "        else:\n",
    "            rows = np.asarray(rows)\n",
    "            assert rows.shape[1] == 2, f\" (patient_id, time)，: {rows.shape}\"\n",
    "            rows_df = pd.DataFrame(rows, columns=[\"patient_id\", \"time\"])\n",
    "        rows_df[\"patient_id\"] = rows_df[\"patient_id\"].astype(int)\n",
    "        def _coerce(x):\n",
    "            if isinstance(x, str):\n",
    "                try:\n",
    "                    dt = datetime.datetime.fromisoformat(x)\n",
    "                except Exception:\n",
    "                    dt = pd.to_datetime(x).to_pydatetime()\n",
    "            else:\n",
    "                dt = pd.to_datetime(x).to_pydatetime()\n",
    "            return dt.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "        rows_df[\"time\"] = rows_df[\"time\"].apply(_coerce)\n",
    "        return feats, rows_df\n",
    "\n",
    "    raise ValueError(f\" CLMBR pkl ：{type(obj)}（：{pkl_path}）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef34682-bd99-4228-a7ad-2ea24e416d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo  # py>=3.9\n",
    "except Exception:\n",
    "    ZoneInfo = None\n",
    "\n",
    "def _timestamp_str():\n",
    "    tz = ZoneInfo(\"Asia/Jakarta\") if ZoneInfo else None\n",
    "    return datetime.now(tz).strftime(\"%Y%m%d-%H%M%S%z\")\n",
    "\n",
    "def _snapshot_lr_params(clf):\n",
    "    \"\"\"\n",
    "     clf（ Pipeline  LogisticRegression ）。\n",
    "    \"\"\"\n",
    "    lr = None\n",
    "    if hasattr(clf, \"named_steps\") and \"logisticregression\" in clf.named_steps:\n",
    "        lr = clf.named_steps[\"logisticregression\"]\n",
    "        scaler = type(clf.named_steps.get(\"maxabsscaler\", None)).__name__ if \"maxabsscaler\" in clf.named_steps else None\n",
    "    else:\n",
    "        lr = clf\n",
    "        scaler = None\n",
    "    snap = {\n",
    "        \"is_pipeline\": hasattr(clf, \"named_steps\"),\n",
    "        \"scaler\": scaler,\n",
    "        \"solver\": getattr(lr, \"solver\", None),\n",
    "        \"penalty\": getattr(lr, \"penalty\", None),\n",
    "        \"C\": getattr(lr, \"C\", None),\n",
    "        \"tol\": getattr(lr, \"tol\", None),\n",
    "        \"max_iter\": getattr(lr, \"max_iter\", None),\n",
    "        \"class_weight\": getattr(lr, \"class_weight\", None),\n",
    "        \"random_state\": getattr(lr, \"random_state\", None),\n",
    "        \"n_iter_\": getattr(lr, \"n_iter_\", None),\n",
    "    }\n",
    "    if hasattr(snap[\"n_iter_\"], \"tolist\"):\n",
    "        snap[\"n_iter_\"] = snap[\"n_iter_\"].tolist()\n",
    "    return snap\n",
    "\n",
    "def _save_run_txt(summary_df, all_results, extra_params=None, out_dir=\"runs\", fname_prefix=\"report\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = _timestamp_str()\n",
    "    path = os.path.join(out_dir, f\"{fname_prefix}-{ts}.txt\")\n",
    "\n",
    "    param_snaps = [r.get(\"param_snapshot\") for r in all_results if r.get(\"param_snapshot\") is not None]\n",
    "    uniq_snaps = []\n",
    "    for p in param_snaps:\n",
    "        if p and p not in uniq_snaps:\n",
    "            uniq_snaps.append(p)\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"===== Run Report =====\\n\")\n",
    "        f.write(f\"time: {ts}\\n\")\n",
    "        if extra_params:\n",
    "            f.write(\"\\n--- run_config ---\\n\")\n",
    "            f.write(json.dumps(extra_params, ensure_ascii=False, indent=2, sort_keys=True))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"\\n--- training_params (unique heads) ---\\n\")\n",
    "        if uniq_snaps:\n",
    "            for i, p in enumerate(uniq_snaps):\n",
    "                f.write(f\"[{i}] {json.dumps(p, ensure_ascii=False, indent=2, sort_keys=True)}\\n\")\n",
    "        else:\n",
    "            f.write(\"(no param_snapshot found in results)\\n\")\n",
    "\n",
    "        f.write(\"\\n--- SUMMARY (DataFrame) ---\\n\")\n",
    "        f.write(summary_df.to_string(index=False))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"\\n--- ALL RESULTS (JSON per row) ---\\n\")\n",
    "        for r in all_results:\n",
    "            f.write(json.dumps(r, ensure_ascii=False, indent=2, sort_keys=True) + \"\\n\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd339f-b4f6-4b6a-b15a-0c8e2e7e5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SplitData:\n",
    "    X:     object  # csr_matrix  ndarray\n",
    "    rows:  pd.DataFrame\n",
    "    idxs:  np.ndarray\n",
    "    y:     np.ndarray  # (0/1)\n",
    "\n",
    "def slice_rows(X, idxs: np.ndarray):\n",
    "    if isinstance(X, csr_matrix):\n",
    "        return X[idxs]\n",
    "    X = np.asarray(X)\n",
    "    return X[idxs]\n",
    "\n",
    "def evaluate_binary(y_true: np.ndarray, prob: np.ndarray, thr: float = 0.5) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    try:\n",
    "        out[\"auroc\"] = float(roc_auc_score(y_true, prob))\n",
    "    except Exception:\n",
    "        out[\"auroc\"] = float(\"nan\")\n",
    "    try:\n",
    "        out[\"auprc\"] = float(average_precision_score(y_true, prob))\n",
    "    except Exception:\n",
    "        out[\"auprc\"] = float(\"nan\")\n",
    "    try:\n",
    "        out[\"brier\"] = float(brier_score_loss(y_true, prob))\n",
    "    except Exception:\n",
    "        out[\"brier\"] = float(\"nan\")\n",
    "    pred = (prob >= thr).astype(int)\n",
    "    out[\"acc@0.5\"] = float(accuracy_score(y_true, pred))\n",
    "    out[\"pos_rate\"] = float(np.mean(y_true))\n",
    "    return out\n",
    "\n",
    "def fit_lr_lbfgs(X_train, y_train) -> LogisticRegression:\n",
    "    model = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=2000,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        class_weight=None,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def fit_lr_saga(X, y, C=1.0, tol=1e-6, max_iter=5000, balanced=False, seed=0):\n",
    "    class_w = \"balanced\" if balanced else None\n",
    "    clf = make_pipeline(\n",
    "        MaxAbsScaler(),\n",
    "        LogisticRegression(\n",
    "            solver=\"saga\", penalty=\"l2\", C=C, tol=tol, max_iter=max_iter,\n",
    "            n_jobs=-1, verbose=0, class_weight=class_w, random_state=seed\n",
    "        )\n",
    "    )\n",
    "    clf.fit(X, y)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba49de-5b4b-4bb3-87d8-01495ce99446",
   "metadata": {},
   "source": [
    "## MultiLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c7dfc-c6cf-4b9f-8e97-5c79c3416af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instances_from_labeled_csv_multilabel(path_to_labels_dir: str, task: str):\n",
    "    lp_csv = os.path.join(path_to_labels_dir, task, \"labeled_patients.csv\")\n",
    "    lp = load_labeled_patients(lp_csv)  # label_type == \"multilabel\"\n",
    "    assert lp.get_labeler_type() == \"multilabel\", \" multilabel\"\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for pid, labels in lp.items():\n",
    "        for lab in labels:\n",
    "            t_iso = lab.time.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "            key = (int(pid), t_iso)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            vals = lab.value\n",
    "            if not isinstance(vals, (list, tuple)):\n",
    "                vals = [vals]\n",
    "            out.append((int(pid), t_iso, [str(v) for v in vals]))\n",
    "    return out\n",
    "\n",
    "def load_label_vocab_from_json(phenotype_json_path: str):\n",
    "    with open(phenotype_json_path, \"r\") as f:\n",
    "        mp = json.load(f)\n",
    "    return list(mp.keys())\n",
    "\n",
    "def _join_rows_multilabel(rows_df: pd.DataFrame,\n",
    "                          samples: list[tuple[int, str, list[str]]],\n",
    "                          label_vocab: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    label_to_idx = {l:i for i,l in enumerate(label_vocab)}\n",
    "    key2row = {(int(pid), t): i for i, (pid, t) in enumerate(zip(rows_df[\"patient_id\"].values,\n",
    "                                                                 rows_df[\"time\"].values))}\n",
    "    idxs, Y, missed, oov = [], [], 0, 0\n",
    "    for pid, t_iso, labels in samples:\n",
    "        r = key2row.get((pid, t_iso))\n",
    "        if r is None:\n",
    "            missed += 1\n",
    "            continue\n",
    "        y = np.zeros(len(label_vocab), dtype=int)\n",
    "        for lab in labels:\n",
    "            j = label_to_idx.get(lab)\n",
    "            if j is None:\n",
    "                oov += 1\n",
    "                continue\n",
    "            y[j] = 1\n",
    "        idxs.append(r)\n",
    "        Y.append(y)\n",
    "    if missed > 0:\n",
    "        logger.warning(f\"[align]  {missed}  rows.csv ，\")\n",
    "    if oov > 0:\n",
    "        logger.warning(f\"[align]  {oov}  label_vocab，\")\n",
    "    return np.array(idxs, dtype=int), (np.vstack(Y) if Y else np.zeros((0, len(label_vocab)), dtype=int))\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "\n",
    "def evaluate_multilabel(Y_true: np.ndarray, P: np.ndarray, thr: float = 0.5) -> dict:\n",
    "    out = {}\n",
    "\n",
    "    # --- AUROC ---\n",
    "    try:\n",
    "        out[\"auroc_micro\"] = float(roc_auc_score(Y_true.ravel(), P.ravel()))\n",
    "    except Exception:\n",
    "        out[\"auroc_micro\"] = float(\"nan\")\n",
    "\n",
    "    per = []\n",
    "    for j in range(Y_true.shape[1]):\n",
    "        yj, pj = Y_true[:, j], P[:, j]\n",
    "        if len(np.unique(yj)) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            per.append(roc_auc_score(yj, pj))\n",
    "        except Exception:\n",
    "            pass\n",
    "    out[\"auroc_macro\"] = float(np.mean(per)) if per else float(\"nan\")\n",
    "\n",
    "    # --- AUPRC ---\n",
    "    try:\n",
    "        out[\"auprc_micro\"] = float(average_precision_score(Y_true.ravel(), P.ravel()))\n",
    "    except Exception:\n",
    "        out[\"auprc_micro\"] = float(\"nan\")\n",
    "\n",
    "    per_ap = []\n",
    "    for j in range(Y_true.shape[1]):\n",
    "        yj, pj = Y_true[:, j], P[:, j]\n",
    "        if len(np.unique(yj)) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            per_ap.append(average_precision_score(yj, pj))\n",
    "        except Exception:\n",
    "            pass\n",
    "    out[\"auprc_macro\"] = float(np.mean(per_ap)) if per_ap else float(\"nan\")\n",
    "\n",
    "    pred = (P >= thr).astype(int)\n",
    "    out[\"f1_micro\"] = float(f1_score(Y_true, pred, average=\"micro\", zero_division=0))\n",
    "    out[\"f1_macro\"] = float(f1_score(Y_true, pred, average=\"macro\", zero_division=0))\n",
    "    out[\"subset_acc\"] = float(accuracy_score(Y_true, pred))\n",
    "\n",
    "    out[\"pos_rate_macro\"] = float(np.mean(Y_true.mean(axis=0))) if Y_true.size else float(\"nan\")\n",
    "    return out\n",
    "\n",
    "def find_best_micro_threshold(Y_val: np.ndarray, P_val: np.ndarray, grid=None) -> float:\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    best_thr, best_f1 = 0.5, -1\n",
    "    for t in grid:\n",
    "        pred = (P_val >= t).astype(int)\n",
    "        f1 = f1_score(Y_val, pred, average=\"micro\", zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, float(t)\n",
    "    return best_thr\n",
    "\n",
    "def find_per_class_thresholds(Y_val: np.ndarray, P_val: np.ndarray, grid=None) -> np.ndarray:\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    T = np.full(Y_val.shape[1], 0.5, dtype=float)\n",
    "    for j in range(Y_val.shape[1]):\n",
    "        yj, pj = Y_val[:, j], P_val[:, j]\n",
    "        if len(np.unique(yj)) < 2:\n",
    "            continue\n",
    "        best, bestf = 0.5, -1\n",
    "        for t in grid:\n",
    "            f1 = f1_score(yj, (pj>=t).astype(int), average=\"binary\", zero_division=0)\n",
    "            if f1 > bestf:\n",
    "                bestf, best = f1, float(t)\n",
    "        T[j] = best\n",
    "    return T\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "\n",
    "def evaluate_multilabel(Y_true: np.ndarray, P: np.ndarray, thr: float = 0.5) -> dict:\n",
    "    out = {}\n",
    "\n",
    "    # --- AUROC ---\n",
    "    try:\n",
    "        out[\"auroc_micro\"] = float(roc_auc_score(Y_true.ravel(), P.ravel()))\n",
    "    except Exception:\n",
    "        out[\"auroc_micro\"] = float(\"nan\")\n",
    "\n",
    "    per = []\n",
    "    for j in range(Y_true.shape[1]):\n",
    "        yj, pj = Y_true[:, j], P[:, j]\n",
    "        if len(np.unique(yj)) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            per.append(roc_auc_score(yj, pj))\n",
    "        except Exception:\n",
    "            pass\n",
    "    out[\"auroc_macro\"] = float(np.mean(per)) if per else float(\"nan\")\n",
    "\n",
    "    # --- AUPRC ---\n",
    "    try:\n",
    "        out[\"auprc_micro\"] = float(average_precision_score(Y_true.ravel(), P.ravel()))\n",
    "    except Exception:\n",
    "        out[\"auprc_micro\"] = float(\"nan\")\n",
    "\n",
    "    per_ap = []\n",
    "    for j in range(Y_true.shape[1]):\n",
    "        yj, pj = Y_true[:, j], P[:, j]\n",
    "        if len(np.unique(yj)) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            per_ap.append(average_precision_score(yj, pj))\n",
    "        except Exception:\n",
    "            pass\n",
    "    out[\"auprc_macro\"] = float(np.mean(per_ap)) if per_ap else float(\"nan\")\n",
    "\n",
    "    pred = (P >= thr).astype(int)\n",
    "    out[\"f1_micro\"] = float(f1_score(Y_true, pred, average=\"micro\", zero_division=0))\n",
    "    out[\"f1_macro\"] = float(f1_score(Y_true, pred, average=\"macro\", zero_division=0))\n",
    "    out[\"subset_acc\"] = float(accuracy_score(Y_true, pred))\n",
    "\n",
    "    out[\"pos_rate_macro\"] = float(np.mean(Y_true.mean(axis=0))) if Y_true.size else float(\"nan\")\n",
    "    return out\n",
    "\n",
    "def find_best_micro_threshold(Y_val: np.ndarray, P_val: np.ndarray, grid=None) -> float:\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    best_thr, best_f1 = 0.5, -1\n",
    "    for t in grid:\n",
    "        pred = (P_val >= t).astype(int)\n",
    "        f1 = f1_score(Y_val, pred, average=\"micro\", zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, float(t)\n",
    "    return best_thr\n",
    "\n",
    "def find_per_class_thresholds(Y_val: np.ndarray, P_val: np.ndarray, grid=None) -> np.ndarray:\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    T = np.full(Y_val.shape[1], 0.5, dtype=float)\n",
    "    for j in range(Y_val.shape[1]):\n",
    "        yj, pj = Y_val[:, j], P_val[:, j]\n",
    "        if len(np.unique(yj)) < 2:\n",
    "            continue\n",
    "        best, bestf = 0.5, -1\n",
    "        for t in grid:\n",
    "            f1 = f1_score(yj, (pj>=t).astype(int), average=\"binary\", zero_division=0)\n",
    "            if f1 > bestf:\n",
    "                bestf, best = f1, float(t)\n",
    "        T[j] = best\n",
    "    return T\n",
    "\n",
    "def run_one_model_multilabel(model_type: str, shots_for_k: dict, label_vocab: list[str], sage=False):\n",
    "    logger.info(f\"=== [{model_type}] (multilabel)  ===\")\n",
    "    if model_type == \"count\":\n",
    "        X_tr, rows_tr, _ = load_count_split(COUNT_FILES[\"train\"])\n",
    "        X_va, rows_va, _ = load_count_split(COUNT_FILES[\"tuning\"])\n",
    "        X_te, rows_te, _ = load_count_split(COUNT_FILES[\"held\"])\n",
    "        X_tr = _ensure_csr32(X_tr); X_va = _ensure_csr32(X_va); X_te = _ensure_csr32(X_te)\n",
    "        scaler = MaxAbsScaler(copy=False); scaler.fit(X_tr)\n",
    "        scale = scaler.scale_.astype(np.float32); inv = np.ones_like(scale, dtype=np.float32)\n",
    "        nz = scale != 0; inv[nz] = 1.0 / scale[nz]\n",
    "        X_tr_s = X_tr.copy(); inplace_column_scale(X_tr_s, inv)\n",
    "        X_va_s = X_va.copy(); inplace_column_scale(X_va_s, inv)\n",
    "        X_te_s = X_te.copy(); inplace_column_scale(X_te_s, inv)\n",
    "    else:\n",
    "        X_tr, rows_tr = load_clmbr_split(CLMBR_FILES[\"train\"])\n",
    "        X_va, rows_va = load_clmbr_split(CLMBR_FILES[\"tuning\"])\n",
    "        X_te, rows_te = load_clmbr_split(CLMBR_FILES[\"held\"])\n",
    "\n",
    "    held_instances = instances_from_labeled_csv_multilabel(HELDOUT[\"labels\"], LABELING_FUNCTION)\n",
    "    idx_te, Y_te = _join_rows_multilabel(rows_te, held_instances, label_vocab)\n",
    "    logger.info(f\"[{model_type}] held_out : {len(idx_te)}/{len(held_instances)}\")\n",
    "\n",
    "    results = []\n",
    "    for rep, payload in shots_for_k.items():\n",
    "        tr_samples = list(zip(payload[\"patient_ids_train_k\"],\n",
    "                              payload[\"label_times_train_k\"],\n",
    "                              payload[\"label_values_train_k\"]))  # <- list[str]\n",
    "        va_samples = list(zip(payload[\"patient_ids_val_k\"],\n",
    "                              payload[\"label_times_val_k\"],\n",
    "                              payload[\"label_values_val_k\"]))    # <- list[str]\n",
    "\n",
    "        idx_tr, Y_tr = _join_rows_multilabel(rows_tr, tr_samples, label_vocab)\n",
    "        idx_va, Y_va = _join_rows_multilabel(rows_va, va_samples, label_vocab)\n",
    "        logger.info(f\"[{model_type}|rep={rep}] train : {len(idx_tr)}/{len(tr_samples)}, \"\n",
    "                    f\"val : {len(idx_va)}/{len(va_samples)}\")\n",
    "\n",
    "        if model_type == \"count\":\n",
    "            Xtr = slice_rows(X_tr_s, idx_tr)\n",
    "            Xva = slice_rows(X_va_s, idx_va)\n",
    "            Xte = slice_rows(X_te_s, idx_te)\n",
    "        else:\n",
    "            Xtr = slice_rows(X_tr, idx_tr)\n",
    "            Xva = slice_rows(X_va, idx_va)\n",
    "            Xte = slice_rows(X_te, idx_te)\n",
    "\n",
    "        base_lr = LogisticRegression(\n",
    "            solver=\"saga\", penalty=\"l2\", C=1.0, tol=1e-6, max_iter=5000,\n",
    "            n_jobs=-1, class_weight=\"balanced\", random_state=rep, verbose=0\n",
    "        )\n",
    "        clf = OneVsRestClassifier(base_lr, n_jobs=-1)\n",
    "        clf.fit(Xtr, Y_tr)\n",
    "\n",
    "        P_tr = clf.predict_proba(Xtr)   # (n_tr, L)\n",
    "        P_va = clf.predict_proba(Xva)   # (n_va, L)\n",
    "        P_te = clf.predict_proba(Xte)   # (n_te, L)\n",
    "\n",
    "        best_thr = find_best_micro_threshold(Y_va, P_va)\n",
    "\n",
    "        m_tr = evaluate_multilabel(Y_tr, P_tr, thr=best_thr)\n",
    "        m_va = evaluate_multilabel(Y_va, P_va, thr=best_thr)\n",
    "        m_te = evaluate_multilabel(Y_te, P_te, thr=best_thr)\n",
    "\n",
    "        logger.success(\n",
    "            f\"[{model_type}|rep={rep}] AUROC micro/macro (tr/va/te) = \"\n",
    "            f\"{m_tr['auroc_micro']:.4f}/{m_tr['auroc_macro']:.4f} | \"\n",
    "            f\"{m_va['auroc_micro']:.4f}/{m_va['auroc_macro']:.4f} | \"\n",
    "            f\"{m_te['auroc_micro']:.4f}/{m_te['auroc_macro']:.4f}  ||  \"\n",
    "            f\"F1@best micro/macro (tr/va/te) = \"\n",
    "            f\"{m_tr['f1_micro']:.3f}/{m_tr['f1_macro']:.3f} | \"\n",
    "            f\"{m_va['f1_micro']:.3f}/{m_va['f1_macro']:.3f} | \"\n",
    "            f\"{m_te['f1_micro']:.3f}/{m_te['f1_macro']:.3f}\"\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"model\": model_type,\n",
    "            \"rep\": rep,\n",
    "            \"metrics_train\": m_tr,\n",
    "            \"metrics_val\": m_va,\n",
    "            \"metrics_test\": m_te,\n",
    "            \"thresholds\": {\"best_on_val_micro\": best_thr},\n",
    "            \"data_stats\": {\n",
    "                \"n_train\": int(Y_tr.shape[0]),\n",
    "                \"pos_train_macro\": float(np.mean(Y_tr.mean(axis=0))),\n",
    "                \"n_val\":   int(Y_va.shape[0]),\n",
    "                \"pos_val_macro\":  float(np.mean(Y_va.mean(axis=0))),\n",
    "                \"n_test\":  int(Y_te.shape[0]),\n",
    "                \"pos_test_macro\": float(np.mean(Y_te.mean(axis=0))),\n",
    "            },\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def _is_multilabel_task(path_to_labels_dir: str, task: str) -> bool:\n",
    "    lp_csv = os.path.join(path_to_labels_dir, task, \"labeled_patients.csv\")\n",
    "    lp = load_labeled_patients(lp_csv)\n",
    "    return lp.get_labeler_type() == \"multilabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad5fb0-f9cd-412c-a4da-1e0b5cb02ed0",
   "metadata": {},
   "source": [
    "## Single Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d86b49-93ac-44fa-887d-f353c973cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.utils.sparsefuncs import inplace_column_scale\n",
    "\n",
    "def _ensure_csr32(X):\n",
    "    if not issparse(X):\n",
    "        X = csr_matrix(X)\n",
    "    X = X.tocsr()\n",
    "    if X.dtype != np.float32:\n",
    "        X = X.astype(np.float32)\n",
    "    return X\n",
    "\n",
    "def fit_lr_saga_staged(X, y, C=1.0, seed=0, max_total=2000, step=200, tol=1e-4, balanced=True, verbose=0):\n",
    "    class_w = \"balanced\" if balanced else None\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"saga\", penalty=\"l2\", C=C,\n",
    "        tol=tol, max_iter=step, warm_start=True,\n",
    "        n_jobs=1, verbose=verbose, class_weight=class_w,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    total = 0\n",
    "    while total < max_total:\n",
    "        clf.fit(X, y)\n",
    "        total += step\n",
    "        if hasattr(clf, \"n_iter_\") and np.all(clf.n_iter_ < clf.max_iter):\n",
    "            break\n",
    "        clf.max_iter += step\n",
    "    return clf\n",
    "\n",
    "def run_one_model(model_type: str, shots_for_k: dict, sage=False):\n",
    "    logger.info(f\"=== [{model_type}]  ===\")\n",
    "    if model_type == \"count\":\n",
    "        X_tr, rows_tr, y_tr_all = load_count_split(COUNT_FILES[\"train\"])\n",
    "        X_va, rows_va, y_va_all = load_count_split(COUNT_FILES[\"tuning\"])\n",
    "        X_te, rows_te, y_te_all = load_count_split(COUNT_FILES[\"held\"])\n",
    "\n",
    "        X_tr = _ensure_csr32(X_tr); X_va = _ensure_csr32(X_va); X_te = _ensure_csr32(X_te)\n",
    "        scaler = MaxAbsScaler(copy=False)\n",
    "        scaler.fit(X_tr)\n",
    "        scale = scaler.scale_.astype(np.float32)\n",
    "        inv = np.ones_like(scale, dtype=np.float32)\n",
    "        nz = scale != 0\n",
    "        inv[nz] = 1.0 / scale[nz]\n",
    "\n",
    "        X_tr_s = X_tr.copy(); inplace_column_scale(X_tr_s, inv)\n",
    "        X_va_s = X_va.copy(); inplace_column_scale(X_va_s, inv)\n",
    "        X_te_s = X_te.copy(); inplace_column_scale(X_te_s, inv)\n",
    "    else:\n",
    "        X_tr, rows_tr = load_clmbr_split(CLMBR_FILES[\"train\"])\n",
    "        X_va, rows_va = load_clmbr_split(CLMBR_FILES[\"tuning\"])\n",
    "        X_te, rows_te = load_clmbr_split(CLMBR_FILES[\"held\"])\n",
    "\n",
    "    held_instances = instances_from_labeled_csv(HELDOUT[\"labels\"], LABELING_FUNCTION)\n",
    "    idx_te, y_te = _join_rows(rows_te, held_instances)\n",
    "    logger.info(f\"[{model_type}] held_out : {len(idx_te)}/{len(held_instances)}\")\n",
    "\n",
    "    results = []\n",
    "    for rep, payload in shots_for_k.items():\n",
    "        tr_samples = list(zip(payload[\"patient_ids_train_k\"],\n",
    "                              payload[\"label_times_train_k\"],\n",
    "                              payload[\"label_values_train_k\"]))\n",
    "        va_samples = list(zip(payload[\"patient_ids_val_k\"],\n",
    "                              payload[\"label_times_val_k\"],\n",
    "                              payload[\"label_values_val_k\"]))\n",
    "        idx_tr, y_tr = _join_rows(rows_tr, tr_samples)\n",
    "        idx_va, y_va = _join_rows(rows_va, va_samples)\n",
    "\n",
    "        logger.info(f\"[{model_type}|rep={rep}] train : {len(idx_tr)}/{len(tr_samples)}, \"\n",
    "                    f\"val : {len(idx_va)}/{len(va_samples)}\")\n",
    "\n",
    "        if model_type == \"count\":\n",
    "            Xtr = slice_rows(X_tr_s, idx_tr)\n",
    "            Xva = slice_rows(X_va_s, idx_va)\n",
    "            Xte = slice_rows(X_te_s, idx_te)\n",
    "        else:\n",
    "            Xtr = slice_rows(X_tr, idx_tr)\n",
    "            Xva = slice_rows(X_va, idx_va)\n",
    "            Xte = slice_rows(X_te, idx_te)\n",
    "\n",
    "        if sage:\n",
    "            print(\"sage\")\n",
    "            C_grid = [0.3, 1.0, 3.0]\n",
    "            best = None\n",
    "            for C in C_grid:\n",
    "                clf_try = fit_lr_saga_staged(Xtr, y_tr, C=C, seed=rep, max_total=2000, step=200, tol=1e-4, balanced=True, verbose=0)\n",
    "                p_va_try = clf_try.predict_proba(Xva)[:, 1]\n",
    "                auroc = roc_auc_score(y_va, p_va_try)\n",
    "                if best is None or auroc > best[0]:\n",
    "                    best = (auroc, C, clf_try)\n",
    "            logger.info(f\"[count|rep={rep}] SAGA best C={best[1]} (AUROC={best[0]:.4f})\")\n",
    "            clf = best[2]\n",
    "        else:\n",
    "            clf = fit_lr_lbfgs(Xtr, y_tr)\n",
    "\n",
    "        p_tr = clf.predict_proba(Xtr)[:, 1]\n",
    "        p_va = clf.predict_proba(Xva)[:, 1]\n",
    "        p_te = clf.predict_proba(Xte)[:, 1]\n",
    "        m_tr = evaluate_binary(y_tr, p_tr)\n",
    "        m_va = evaluate_binary(y_va, p_va)\n",
    "        m_te = evaluate_binary(y_te, p_te)\n",
    "\n",
    "        best_thr, _ = find_best_f1_threshold(y_va, p_va)\n",
    "        f1_05_tr = f1_at_threshold(y_tr, p_tr, 0.5)\n",
    "        f1_05_va = f1_at_threshold(y_va, p_va, 0.5)\n",
    "        f1_05_te = f1_at_threshold(y_te, p_te, 0.5)\n",
    "        f1b_tr = f1_at_threshold(y_tr, p_tr, best_thr)\n",
    "        f1b_va = f1_at_threshold(y_va, p_va, best_thr)\n",
    "        f1b_te = f1_at_threshold(y_te, p_te, best_thr)\n",
    "\n",
    "        logger.success(\n",
    "            f\"[{model_type}|rep={rep}] \"\n",
    "            f\"AUROC (tr/va/te) = {m_tr['auroc']:.4f} / {m_va['auroc']:.4f} / {m_te['auroc']:.4f}  | \"\n",
    "            f\"AUPRC (tr/va/te) = {m_tr['auprc']:.4f} / {m_va['auprc']:.4f} / {m_te['auprc']:.4f}  | \"\n",
    "            f\"F1@0.5 (tr/va/te) = {f1_05_tr['f1']:.3f} / {f1_05_va['f1']:.3f} / {f1_05_te['f1']:.3f}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"[{model_type}|rep={rep}] F1@val-best({best_thr:.3f}) \"\n",
    "            f\"(tr/va/te) = {f1b_tr['f1']:.3f} / {f1b_va['f1']:.3f} / {f1b_te['f1']:.3f}  | \"\n",
    "            f\"P/R@test = {f1b_te['precision']:.3f}/{f1b_te['recall']:.3f}\"\n",
    "        )\n",
    "\n",
    "        param_snapshot = _snapshot_lr_params(clf)\n",
    "        results.append({\n",
    "            \"model\": model_type,\n",
    "            \"rep\": rep,\n",
    "            \"metrics_train\": m_tr,\n",
    "            \"metrics_val\": m_va,\n",
    "            \"metrics_test\": m_te,\n",
    "            \"thresholds\": {\"fixed_0.5\": 0.5, \"best_on_val\": best_thr},\n",
    "            \"f1_at_0.5\": {\n",
    "                \"train\": {\"f1\": f1_05_tr[\"f1\"], \"precision\": f1_05_tr[\"precision\"], \"recall\": f1_05_tr[\"recall\"]},\n",
    "                \"val\":   {\"f1\": f1_05_va[\"f1\"], \"precision\": f1_05_va[\"precision\"], \"recall\": f1_05_va[\"recall\"]},\n",
    "                \"test\":  {\"f1\": f1_05_te[\"f1\"], \"precision\": f1_05_te[\"precision\"], \"recall\": f1_05_te[\"recall\"]},\n",
    "            },\n",
    "            \"f1_at_best\": {\n",
    "                \"train\": {\"f1\": f1b_tr[\"f1\"], \"precision\": f1b_tr[\"precision\"], \"recall\": f1b_tr[\"recall\"]},\n",
    "                \"val\":   {\"f1\": f1b_va[\"f1\"], \"precision\": f1b_va[\"precision\"], \"recall\": f1b_va[\"recall\"]},\n",
    "                \"test\":  {\"f1\": f1b_te[\"f1\"], \"precision\": f1b_te[\"precision\"], \"recall\": f1b_te[\"recall\"]},\n",
    "            },\n",
    "            \"param_snapshot\": param_snapshot,\n",
    "            \"data_stats\": {\n",
    "                \"n_train\": int(len(y_tr)), \"pos_train\": int(y_tr.sum()),\n",
    "                \"n_val\":   int(len(y_va)), \"pos_val\":  int(y_va.sum()),\n",
    "                \"n_test\":  int(len(y_te)), \"pos_test\": int(y_te.sum()),\n",
    "            },\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31cbae6-7549-44d3-bb98-b2342d956ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "assert os.path.exists(SHOTS_JSON), f\" shots JSON: {SHOTS_JSON}\"\n",
    "shots = load_shots(SHOTS_JSON, LABELING_FUNCTION, K_VALUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e93ea-0e7c-4613-8370-804883aaf077",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559dcfa-24b7-4735-b12e-8bbfaf7fda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHENOTYPE_JSON_PATH = \"/root/autodl-tmp/phenotypes_ccs_from_parent.json\"\n",
    "def eval_one_model(model_name: str, save_txt: bool = True, out_dir: str = \"runs\", sage=False):\n",
    "    all_results = []\n",
    "    IS_MULTI = _is_multilabel_task(HELDOUT[\"labels\"], LABELING_FUNCTION)\n",
    "    if IS_MULTI:\n",
    "        LABEL_VOCAB = load_label_vocab_from_json(PHENOTYPE_JSON_PATH)\n",
    "    for k, reps in shots.items():\n",
    "        logger.info(f\"******** K={k} | model={model_name} | task={'multilabel' if IS_MULTI else 'binary'} ********\")\n",
    "        if IS_MULTI:\n",
    "            res = run_one_model_multilabel(model_name, reps, label_vocab=LABEL_VOCAB, sage=sage)\n",
    "        else:\n",
    "            res = run_one_model(model_name, reps, sage=sage)\n",
    "        for r in res:\n",
    "            r[\"k\"] = k\n",
    "            r[\"model\"] = model_name\n",
    "        all_results.extend(res)\n",
    "\n",
    "    rows = []\n",
    "    for r in all_results:\n",
    "        te = r[\"metrics_test\"]; va = r[\"metrics_val\"]\n",
    "        if IS_MULTI:\n",
    "            rows.append({\n",
    "                \"k\": r[\"k\"], \"model\": r[\"model\"], \"rep\": r[\"rep\"],\n",
    "                \"val_auroc_micro\": va[\"auroc_micro\"], \"val_auroc_macro\": va[\"auroc_macro\"],\n",
    "                \"val_auprc_micro\": va[\"auprc_micro\"], \"val_auprc_macro\": va[\"auprc_macro\"],\n",
    "                \"test_auroc_micro\": te[\"auroc_micro\"], \"test_auroc_macro\": te[\"auroc_macro\"],\n",
    "                \"test_auprc_micro\": te[\"auprc_micro\"], \"test_auprc_macro\": te[\"auprc_macro\"],\n",
    "                \"test_subset_acc\": te[\"subset_acc\"],\n",
    "                \"val_f1_micro\": va[\"f1_micro\"], \"test_f1_micro\": te[\"f1_micro\"],\n",
    "                \"val_f1_macro\": va[\"f1_macro\"], \"test_f1_macro\": te[\"f1_macro\"],\n",
    "                \"best_thr\": r[\"thresholds\"][\"best_on_val_micro\"],\n",
    "            })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"k\": r[\"k\"], \"model\": r[\"model\"], \"rep\": r[\"rep\"],\n",
    "                \"val_auroc\": va[\"auroc\"], \"val_auprc\": va[\"auprc\"], \"val_brier\": va[\"brier\"],\n",
    "                \"test_auroc\": te[\"auroc\"], \"test_auprc\": te[\"auprc\"], \"test_brier\": te[\"brier\"],\n",
    "                \"test_acc@0.5\": te[\"acc@0.5\"], \"test_pos_rate\": te[\"pos_rate\"],\n",
    "                \"val_f1@best\": r[\"f1_at_best\"][\"val\"][\"f1\"], \"test_f1@best\": r[\"f1_at_best\"][\"test\"][\"f1\"],\n",
    "                \"best_thr\":     r[\"thresholds\"][\"best_on_val\"],\n",
    "            })\n",
    "    summary = pd.DataFrame(rows).sort_values(by=[\"k\",\"model\",\"rep\"]).reset_index(drop=True)\n",
    "    print(f\"\\n===== Summary for model: {model_name} =====\")\n",
    "    print(summary)\n",
    "\n",
    "    if save_txt:\n",
    "        extra = {\"model\": model_name, \"k_values\": list(shots.keys()), \"multilabel\": IS_MULTI}\n",
    "        out_fp = _save_run_txt(summary, all_results, extra_params=extra, out_dir=out_dir, fname_prefix=f\"{model_name}\")\n",
    "        print(f\"[Saved] TXT report -> {out_fp}\")\n",
    "\n",
    "    return summary, all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82e2b2-e1a1-4b1c-8eb7-2cb3980a85a9",
   "metadata": {},
   "source": [
    "## Count Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c6e5d-8187-4445-bcf7-da3226edbaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_m0 = eval_one_model(MODELS[0],sage=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a167f-b10b-416d-972d-03cc43be4e00",
   "metadata": {},
   "source": [
    "## CLMBR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578cbae-8df0-443d-ab3b-5bdf429f4d7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_m1 = eval_one_model(MODELS[1],sage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287c340-ae1f-420c-8e14-f0df8d269bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_results = []\n",
    "for k, reps in shots.items():\n",
    "    logger.info(f\"******** K={k}（{'ALL' if k==-1 else 'FEW'}）********\")\n",
    "    for m in MODELS:\n",
    "        res = run_one_model(m, reps)\n",
    "        for r in res:\n",
    "            r[\"k\"] = k\n",
    "        all_results.extend(res)\n",
    "\n",
    "rows = []\n",
    "for r in all_results:\n",
    "    rows.append({\n",
    "        \"k\": r[\"k\"],\n",
    "        \"model\": r[\"model\"],\n",
    "        \"rep\": r[\"rep\"],\n",
    "        \"val_auroc\": r[\"metrics_val\"][\"auroc\"],\n",
    "        \"val_auprc\": r[\"metrics_val\"][\"auprc\"],\n",
    "        \"val_brier\": r[\"metrics_val\"][\"brier\"],\n",
    "        \"test_auroc\": r[\"metrics_test\"][\"auroc\"],\n",
    "        \"test_auprc\": r[\"metrics_test\"][\"auprc\"],\n",
    "        \"test_brier\": r[\"metrics_test\"][\"brier\"],\n",
    "        \"test_acc@0.5\": r[\"metrics_test\"][\"acc@0.5\"],\n",
    "        \"test_pos_rate\": r[\"metrics_test\"][\"pos_rate\"],\n",
    "    })\n",
    "summary = pd.DataFrame(rows).sort_values(by=[\"k\",\"model\",\"rep\"]).reset_index(drop=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbacb4-9a8d-4f03-a6b9-fb738a0a0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, numpy as np, pandas as pd\n",
    "\n",
    "PKL = \"/root/autodl-tmp/femr/train/femr_features/count_features.pkl\"\n",
    "\n",
    "with open(PKL, \"rb\") as f:\n",
    "    obj = pickle.load(f)\n",
    "\n",
    "print(\"TOP:\", type(obj))\n",
    "if isinstance(obj, dict):\n",
    "    print(\"  keys:\", list(obj.keys())[:50])\n",
    "    for k,v in obj.items():\n",
    "        if isinstance(v, (np.ndarray, pd.DataFrame, list, tuple)):\n",
    "            try:\n",
    "                shape = v.shape if hasattr(v, \"shape\") else len(v)\n",
    "            except Exception:\n",
    "                shape = \"?\"\n",
    "            print(f\"   - {k}: {type(v)}, shape/len={shape}\")\n",
    "        else:\n",
    "            print(f\"   - {k}: {type(v)}\")\n",
    "\n",
    "elif isinstance(obj, (list, tuple)):\n",
    "    print(\"  len =\", len(obj))\n",
    "    for i,v in enumerate(obj[:10]):\n",
    "        if isinstance(v, (np.ndarray, pd.DataFrame, list, tuple)):\n",
    "            try:\n",
    "                shape = v.shape if hasattr(v, \"shape\") else len(v)\n",
    "            except Exception:\n",
    "                shape = \"?\"\n",
    "            print(f\"   [{i}] {type(v)}, shape/len={shape}\")\n",
    "        else:\n",
    "            print(f\"   [{i}] {type(v)}\")\n",
    "\n",
    "else:\n",
    "    print(\"  (unknown top-level)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ehr)",
   "language": "python",
   "name": "ehr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}