{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate few-shot / subsample splits: ICU phenotyping\n\n## Purpose\nCreate few-shot subsets/splits for ICU phenotyping experiments.\n\n## Inputs\n- Labels and/or features produced earlier\n\n## Outputs\n- Shot/split files saved to your chosen output directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911b24e-27bf-4433-bad8-f8c4df0fcaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import os, csv, json, random\n",
    "from typing import Dict, List, Tuple\n",
    "from ehrshot.labelers.core import load_labeled_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01f10c-0a09-4ebe-981f-2684f27749ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/root/autodl-tmp/femr\"\n",
    "LABELING_FUNCTION = \"mimic_icu_phenotyping\"\n",
    "\n",
    "TRAIN_LABELS_DIR   = f\"{ROOT}/train/femr_labels\"\n",
    "TUNING_LABELS_DIR  = f\"{ROOT}/tuning/femr_labels\"\n",
    "HELDOUT_LABELS_DIR = f\"{ROOT}/held_out/femr_labels\"  # #6JSON， split.csv\n",
    "\n",
    "TRAIN_SPLIT_CSV   = f\"{ROOT}/train/split.csv\"\n",
    "TUNING_SPLIT_CSV  = f\"{ROOT}/tuning/split.csv\"\n",
    "HELDOUT_SPLIT_CSV = f\"{ROOT}/held_out/split.csv\"\n",
    "\n",
    "SHOT_STRAT = \"few\"  # \"all\"  \"few\"\n",
    "KS = [1, 8, 16, 32, 64, 128, -1]  # SHOT_STRAT=\"few\"\n",
    "N_REPLICATES = 5\n",
    "SEED = 97\n",
    "BALANCE = \"balanced\"  # \"balanced\" =  1:1 ；\"prevalence\" =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33f719-0408-4ca9-9fe3-9e3019c91a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_split_csv_from_labeled(lp_path: str, out_csv: str, split_name: str):\n",
    "    \"\"\" labeled_patients.csv  patient_id  split（train/val/test）\"\"\"\n",
    "    lp = load_labeled_patients(lp_path)\n",
    "\n",
    "    pid_set = set()\n",
    "    for pid, _labels in lp.items():\n",
    "        pid_set.add(int(pid))\n",
    "    pids = sorted(pid_set)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"patient_id\", \"split\"])\n",
    "        for pid in pids:\n",
    "            w.writerow([pid, split_name])\n",
    "\n",
    "    logger.info(f\"[split.csv] {out_csv}: {split_name} = {len(pids)} patients\")\n",
    "    \n",
    "def _pick_label_csv(base_dir: str, task: str) -> str:\n",
    "    p_all = os.path.join(base_dir, task, \"all_labels.csv\")\n",
    "    p_lp  = os.path.join(base_dir, task, \"labeled_patients.csv\")\n",
    "    if os.path.exists(p_all):\n",
    "        return p_all\n",
    "    if os.path.exists(p_lp):\n",
    "        return p_lp\n",
    "    raise FileNotFoundError(f\" {p_all}  {p_lp}\")\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Any, Iterable, Dict, List, Tuple\n",
    "import random, json, csv, os\n",
    "from ehrshot.labelers.core import load_labeled_patients\n",
    "\n",
    "def _load_instances_any(lp_path: str) -> Tuple[List[Tuple[int, str, Any]], str]:\n",
    "    lp = load_labeled_patients(lp_path)\n",
    "    ltype = getattr(lp, \"labeler_type\", \"unknown\")\n",
    "    inst, seen = [], set()\n",
    "    for pid, labels in lp.items():\n",
    "        for lab in labels:\n",
    "            t_iso = lab.time.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "            key = (int(pid), t_iso)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            v = lab.value  # ：bool/int/str/list[str]\n",
    "            inst.append((int(pid), t_iso, v))\n",
    "    return inst, ltype\n",
    "\n",
    "def _orders_boolean(train_list, seed):\n",
    "    pos = [i for i,(_,_,v) in enumerate(train_list) if bool(v)]\n",
    "    neg = [i for i,(_,_,v) in enumerate(train_list) if not bool(v)]\n",
    "    rng = random.Random(seed); rng.shuffle(pos); rng.shuffle(neg)\n",
    "    return pos, neg\n",
    "\n",
    "def _orders_categorical(train_list, seed):\n",
    "    cls2idxs: Dict[str, List[int]] = defaultdict(list)\n",
    "    for i,(_,_,v) in enumerate(train_list):\n",
    "        cls2idxs[str(v)].append(i)\n",
    "    rng = random.Random(seed)\n",
    "    for idxs in cls2idxs.values():\n",
    "        rng.shuffle(idxs)\n",
    "    return cls2idxs  # dict[str, List[int]]\n",
    "\n",
    "def _order_multilabel(train_list, seed):\n",
    "    labels_per_inst: List[List[str]] = []\n",
    "    for _,_,v in train_list:\n",
    "        if v is None:\n",
    "            labels_per_inst.append([])\n",
    "        elif isinstance(v, (list, set, tuple)):\n",
    "            labels_per_inst.append([str(x) for x in v])\n",
    "        else:\n",
    "            labels_per_inst.append([str(v)])  # ：\n",
    "\n",
    "    freq = Counter()\n",
    "    for tags in labels_per_inst:\n",
    "        for t in set(tags):  # set ，\n",
    "            freq[t] += 1\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    n = len(train_list)\n",
    "    idxs = list(range(n))\n",
    "    rng.shuffle(idxs)\n",
    "\n",
    "    def rarity_score(i: int) -> float:\n",
    "        tags = labels_per_inst[i]\n",
    "        if not tags:\n",
    "            return 0.0\n",
    "        return sum(1.0 / max(1, freq[t]) for t in set(tags)) + 1e-6 * rng.random()\n",
    "\n",
    "    idxs.sort(key=rarity_score, reverse=True)\n",
    "    return idxs\n",
    "\n",
    "def _extend_to_k(order: List[int], k: int) -> List[int]:\n",
    "    if k <= 0: return order[:]\n",
    "    if not order: return []\n",
    "    if len(order) >= k: return order[:k]\n",
    "    reps, rem = divmod(k, len(order))\n",
    "    return order * reps + order[:rem]\n",
    "\n",
    "def _build_train_indices_boolean(pos_order, neg_order, k, balance=\"balanced\"):\n",
    "    if k <= 0:\n",
    "        return pos_order + neg_order\n",
    "    npos, nneg = len(pos_order), len(neg_order)\n",
    "    if balance == \"balanced\":\n",
    "        k_pos = k // 2; k_neg = k - k_pos\n",
    "        if npos == 0: k_pos, k_neg = 0, k\n",
    "        if nneg == 0: k_pos, k_neg = k, 0\n",
    "    else:\n",
    "        tot = max(1, npos + nneg); prop = npos / tot\n",
    "        k_pos = int(round(k * prop)); k_neg = k - k_pos\n",
    "    return _extend_to_k(pos_order, k_pos) + _extend_to_k(neg_order, k_neg)\n",
    "\n",
    "def _build_train_indices_categorical(cls2idxs: Dict[str,List[int]], k: int, balance=\"balanced\"):\n",
    "    orders = {c: idxs[:] for c, idxs in cls2idxs.items()}\n",
    "    classes = list(orders.keys())\n",
    "    if k <= 0:\n",
    "        merged = []\n",
    "        for c in classes:\n",
    "            merged += orders[c]\n",
    "        return merged\n",
    "    C = max(1, len(classes))\n",
    "    sizes = {c: len(orders[c]) for c in classes}\n",
    "    if balance == \"balanced\":\n",
    "        k_each = {c: k // C for c in classes}\n",
    "        rem = k - sum(k_each.values())\n",
    "        for c in sorted(classes, key=lambda x: -sizes[x])[:rem]:\n",
    "            k_each[c] += 1\n",
    "    else:\n",
    "        tot = sum(sizes.values()) or 1\n",
    "        k_each = {c: int(round(k * sizes[c] / tot)) for c in classes}\n",
    "        diff = k - sum(k_each.values())\n",
    "        if diff != 0:\n",
    "            for c in sorted(classes, key=lambda x: -sizes[x]):\n",
    "                if diff == 0: break\n",
    "                k_each[c] += 1 if diff > 0 else -1\n",
    "                diff += -1 if diff > 0 else 1\n",
    "    merged = []\n",
    "    for c in classes:\n",
    "        merged += _extend_to_k(orders[c], k_each[c])\n",
    "    return merged\n",
    "\n",
    "def _build_train_indices_multilabel(order_all: List[int], k: int):\n",
    "    if k <= 0: return order_all[:]\n",
    "    return order_all[:k]\n",
    "\n",
    "def _attach_row_indices(entry: dict, row_index_map: Dict[str,int], split: str):\n",
    "    key_pid = f\"patient_ids_{split}_k\"\n",
    "    key_tim = f\"label_times_{split}_k\"\n",
    "    rows = []\n",
    "    for pid, t_iso in zip(entry[key_pid], entry[key_tim]):\n",
    "        rid = row_index_map.get(f\"{int(pid)}|{t_iso}\")\n",
    "        rows.append(None if rid is None else int(rid))\n",
    "    entry[f\"row_indices_{split}_k\"] = rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859920f0-5efc-42e3-8354-a0cf361d886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lp   = _pick_label_csv(TRAIN_LABELS_DIR,   LABELING_FUNCTION)\n",
    "tuning_lp  = _pick_label_csv(TUNING_LABELS_DIR,  LABELING_FUNCTION)\n",
    "heldout_lp = _pick_label_csv(HELDOUT_LABELS_DIR, LABELING_FUNCTION)\n",
    "\n",
    "_write_split_csv_from_labeled(train_lp,   TRAIN_SPLIT_CSV,   \"train\")\n",
    "_write_split_csv_from_labeled(tuning_lp,  TUNING_SPLIT_CSV,  \"val\")\n",
    "_write_split_csv_from_labeled(heldout_lp, HELDOUT_SPLIT_CSV, \"test\")\n",
    "\n",
    "train_pool, train_type = _load_instances_any(train_lp)   # [(pid, time_iso, value), ...]\n",
    "val_pool,   val_type   = _load_instances_any(tuning_lp)\n",
    "assert train_type == val_type, f\"train={train_type}, val={val_type} \"\n",
    "labeler_type = train_type\n",
    "logger.info(f\" {labeler_type} \")\n",
    "\n",
    "k_list = ([-1] if SHOT_STRAT == \"all\" else sorted(KS))\n",
    "few_shots_dict = { LABELING_FUNCTION: {} }\n",
    "\n",
    "maybe_row_index_json = os.path.join(TRAIN_LABELS_DIR, LABELING_FUNCTION, \"..\", \"..\", \"features\", LABELING_FUNCTION, \"row_index.json\")\n",
    "row_index_map = {}\n",
    "if os.path.exists(maybe_row_index_json):\n",
    "    try:\n",
    "        with open(maybe_row_index_json, \"r\") as f:\n",
    "            row_index_map = json.load(f)\n",
    "        logger.info(f\"[row_index] loaded: {maybe_row_index_json}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"[row_index] load failed: {e}; will skip row indices\")\n",
    "\n",
    "train_pids  = [p for p,_,_ in train_pool]\n",
    "train_times = [t for _,t,_ in train_pool]\n",
    "train_vals  = [v for _,_,v in train_pool]\n",
    "val_pids    = [p for p,_,_ in val_pool]\n",
    "val_times   = [t for _,t,_ in val_pool]\n",
    "val_vals    = [v for _,_,v in val_pool]\n",
    "\n",
    "for k in k_list:\n",
    "    few_shots_dict[LABELING_FUNCTION][k] = {}\n",
    "    for rep in range(N_REPLICATES):\n",
    "        if labeler_type in (\"boolean\",):\n",
    "            pos_order, neg_order = _orders_boolean(train_pool, SEED + rep)\n",
    "            train_idxs = _build_train_indices_boolean(pos_order, neg_order, k, BALANCE)\n",
    "        elif labeler_type in (\"categorical\", \"string\"):\n",
    "            cls2idxs = _orders_categorical(train_pool, SEED + rep)\n",
    "            train_idxs = _build_train_indices_categorical(cls2idxs, k, BALANCE)\n",
    "        else:\n",
    "            order_all = _order_multilabel(train_pool, SEED + rep)\n",
    "            train_idxs = _build_train_indices_multilabel(order_all, k)\n",
    "\n",
    "        val_idxs = list(range(len(val_pool)))\n",
    "\n",
    "        entry = {\n",
    "            \"labeler_type\":          labeler_type,\n",
    "            \"patient_ids_train_k\":   [train_pids[i]  for i in train_idxs],\n",
    "            \"patient_ids_val_k\":     [val_pids[i]    for i in val_idxs],\n",
    "            \"label_times_train_k\":   [train_times[i] for i in train_idxs],\n",
    "            \"label_times_val_k\":     [val_times[i]   for i in val_idxs],\n",
    "            \"label_values_train_k\":  [train_vals[i]  for i in train_idxs],\n",
    "            \"label_values_val_k\":    [val_vals[i]    for i in val_idxs],\n",
    "            \"train_idxs\":            train_idxs,\n",
    "            \"val_idxs\":              val_idxs,\n",
    "        }\n",
    "\n",
    "        if row_index_map:\n",
    "            _attach_row_indices(entry, row_index_map, split=\"train\")\n",
    "            _attach_row_indices(entry, row_index_map, split=\"val\")\n",
    "\n",
    "        few_shots_dict[LABELING_FUNCTION][k][rep] = entry\n",
    "\n",
    "out_json = os.path.join(TRAIN_LABELS_DIR, LABELING_FUNCTION, f\"{SHOT_STRAT}_shots_data.json\")\n",
    "os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(few_shots_dict, f, ensure_ascii=False)\n",
    "logger.success(f\"[ok] wrote {out_json}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ehr)",
   "language": "python",
   "name": "ehr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}