{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build FEMR database (EHRSHOT-style) from MEDS/OMOP outputs\n",
    "\n",
    "## Purpose\n",
    "Create an EHRSHOT/FEMR-style event stream and build the FEMR patient database for downstream labeling, featurization, and modeling. This is where the missing-rate threshold is applied (code filtering).\n",
    "\n",
    "## Inputs\n",
    "- MEDS/OMOP-derived patient parquet files\n",
    "- Output directory root containing train/tuning/held_out splits\n",
    "\n",
    "## Outputs\n",
    "- Per-split FEMR database under: <BASE>/<split>/extract/\n",
    "- Intermediate event CSV(s) used by etl_simple_femr\n",
    "- Code-frequency / missing-rate filtering artifacts (if enabled)\n",
    "\n",
    "## Notes\n",
    "To run multiple missing-rate thresholds τ, use a different <BASE> (or output folder) per τ to avoid overwriting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afca36a-cee7-4792-9a2d-6b76ed684759",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0.Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a326f-f81b-405d-9380-0230706d5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# !pip install femr loguru tqdm pyarrow pandas\n",
    "import femr.datasets\n",
    "from femr import Patient, Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a89c5-3baf-4384-92fc-1fdf00aeb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_MEDS_DIR = \"/root/code/MEDS_Process/MEDS_to_OMOP/data/held_out\" \n",
    "\n",
    "PATH_TO_OUTPUT_DIR = \"/root/autodl-tmp/femr/held_out\"\n",
    "\n",
    "PATH_TO_TEMP_DIR = \"/root/autodl-tmp/held_out/temp_patient_data\"\n",
    "\n",
    "NUM_PROCESSES = 15\n",
    "\n",
    "\n",
    "print(f\": {PATH_TO_MEDS_DIR}\")\n",
    "print(f\": {PATH_TO_OUTPUT_DIR}\")\n",
    "print(f\": {PATH_TO_TEMP_DIR}\")\n",
    "print(f\": {NUM_PROCESSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ea76f-1c5f-45e9-8a36-0757c1a1a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        \n",
    "        return len(files)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"： '{directory_path}'\")\n",
    "        return 0\n",
    "\n",
    "directory_path = PATH_TO_TEMP_DIR\n",
    "file_count = count_files_in_directory(directory_path)\n",
    "print(f\" '{directory_path}' : {file_count}\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def list_first_10_files(directory_path):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        \n",
    "        return files[:10]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"： '{directory_path}'\")\n",
    "        return []\n",
    "\n",
    "first_10_files = list_first_10_files(directory_path)\n",
    "\n",
    "if first_10_files:\n",
    "    print(f\" '{directory_path}'  10 ：\")\n",
    "    for file in first_10_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(f\"。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358126d-62db-41d2-992e-d534b83d0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/root/autodl-tmp/tuning/temp_patient_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3ed1d-8d79-4fd5-a82f-1c6a1d69ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_file_in_directory(directory_path, filename):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "filename = \"10040908.parquet\"\n",
    "\n",
    "if check_file_in_directory(directory_path, filename):\n",
    "    print(f\" '{filename}'  '{directory_path}' 。\")\n",
    "else:\n",
    "    print(f\" '{filename}'  '{directory_path}' 。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615aa8f-f558-4071-ae49-89a18c64019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        \n",
    "        return len(files)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"： '{directory_path}'\")\n",
    "        return 0\n",
    "\n",
    "directory_path = PATH_TO_TEMP_DIR\n",
    "file_count = count_files_in_directory(directory_path)\n",
    "print(f\" '{directory_path}' : {file_count}\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def list_first_10_files(directory_path):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "        \n",
    "        return files[:10]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"： '{directory_path}'\")\n",
    "        return []\n",
    "\n",
    "first_10_files = list_first_10_files(directory_path)\n",
    "\n",
    "if first_10_files:\n",
    "    print(f\" '{directory_path}'  10 ：\")\n",
    "    for file in first_10_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(f\"。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978ca12-f5e4-4a56-8bbe-12daa230adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_file(patient_file_path: str) -> Patient | None:\n",
    "    \"\"\"\n",
    "    ParquetFEMR Patient。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        patient_id = int(os.path.basename(patient_file_path).split('.')[0])\n",
    "        \n",
    "        df = pd.read_parquet(patient_file_path)\n",
    "        \n",
    "        rows = df.to_dict('records')\n",
    "        \n",
    "        events = []\n",
    "        for row in rows:\n",
    "            code = row.get(\"code\")\n",
    "            if code is None or code == '':\n",
    "                continue\n",
    "            \n",
    "            start_time = row[\"time\"]\n",
    "            value = row.get(\"numeric_value\")\n",
    "            if pd.isna(value):\n",
    "                value = None\n",
    "            \n",
    "            omop_table = row.get(\"ordercategorydescription\", \"MIMIC_MEDS\")\n",
    "            event = Event(start=start_time, code=str(code), value=value, omop_table=omop_table)\n",
    "            events.append(event)\n",
    "        \n",
    "        if not events:\n",
    "            return None\n",
    "            \n",
    "        events.sort(key=lambda x: x.start)\n",
    "        return Patient(patient_id=patient_id, events=events)\n",
    "    except Exception as e:\n",
    "        logger.error(f\" {patient_file_path} : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea63eb4-72f6-4f79-9ff0-9ec3e3a37080",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Step 1: create patient.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8c126-33c3-4a41-b885-ad75aca265c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger.info(\"---  1:  ---\")\n",
    "if os.path.exists(PATH_TO_TEMP_DIR):\n",
    "    shutil.rmtree(PATH_TO_TEMP_DIR)\n",
    "os.makedirs(PATH_TO_TEMP_DIR)\n",
    "\n",
    "parquet_files = [os.path.join(PATH_TO_MEDS_DIR, f) for f in os.listdir(PATH_TO_MEDS_DIR) if f.endswith('.parquet')]\n",
    "REQUIRED_COLUMNS = ['subject_id', 'time', 'code', 'numeric_value', 'ordercategorydescription']\n",
    "\n",
    "for file_path in tqdm(parquet_files, desc=\"\"):\n",
    "    df = pd.read_parquet(file_path, columns=REQUIRED_COLUMNS)\n",
    "    for subject_id, group in df.groupby('subject_id'):\n",
    "        patient_file = os.path.join(PATH_TO_TEMP_DIR, f\"{subject_id}.parquet\")\n",
    "        if os.path.exists(patient_file):\n",
    "            existing_data = pd.read_parquet(patient_file)\n",
    "            combined_data = pd.concat([existing_data, group], ignore_index=True)\n",
    "            combined_data.to_parquet(patient_file, index=False)\n",
    "        else:\n",
    "            group.to_parquet(patient_file, index=False)\n",
    "\n",
    "logger.success(\"---  1:  ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa363c47-3882-43ff-812b-4affd4d82a86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2: Transfer all patient.parquet to one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1259bf5-4e82-45cc-9628-668e80d84342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd  # pandas\n",
    "\n",
    "def process_patient_file_to_csv_rows(patient_file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Parquet，CSV。\n",
    "     femr : [patient_id, start, code, value]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        patient_id = int(os.path.basename(patient_file_path).split('.')[0])\n",
    "        df = pd.read_parquet(patient_file_path)\n",
    "        \n",
    "        df = df.dropna(subset=['code'])\n",
    "        if df.empty:\n",
    "            return []\n",
    "\n",
    "        df['patient_id'] = patient_id\n",
    "        df['time'] = df['time'].apply(lambda x: x.isoformat())\n",
    "        df['numeric_value'] = df['numeric_value'].where(pd.notna(df['numeric_value']), None)\n",
    "\n",
    "        df.rename(columns={'time': 'start', 'numeric_value': 'value'}, inplace=True)\n",
    "        \n",
    "        output_df = df[['patient_id', 'start', 'code', 'value']]\n",
    "        \n",
    "        return output_df.values.tolist()\n",
    "    except Exception as e:\n",
    "        logger.error(f\" {patient_file_path} : {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be8c31-b557-4f5f-82f3-83cff50e5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil  # shutil\n",
    "\n",
    "\n",
    "logger.info(\"---  2:  mimiciv.csv () ---\")\n",
    "\n",
    "EVENTS_CSV_PATH = os.path.join(PATH_TO_OUTPUT_DIR, \"mimiciv.csv\")\n",
    "\n",
    "if not os.path.exists(PATH_TO_OUTPUT_DIR):\n",
    "    os.makedirs(PATH_TO_OUTPUT_DIR)\n",
    "\n",
    "patient_files_to_process = [os.path.join(PATH_TO_TEMP_DIR, f) for f in os.listdir(PATH_TO_TEMP_DIR) if f.endswith('.parquet')]\n",
    "\n",
    "with open(EVENTS_CSV_PATH, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(['patient_id', 'start', 'code', 'value'])\n",
    "    \n",
    "    with Pool(processes=NUM_PROCESSES) as pool:\n",
    "        results_iterator = pool.imap_unordered(process_patient_file_to_csv_rows, patient_files_to_process)\n",
    "        for rows in tqdm(results_iterator, total=len(patient_files_to_process), desc=\"CSV\"):\n",
    "            if rows:\n",
    "                writer.writerows(rows)\n",
    "\n",
    "logger.success(f\"---  2:  mimiciv.csv ，: {EVENTS_CSV_PATH} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b96ee-9f3e-4d51-929f-866499067bcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 3: Data cleaning on the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f79763-4507-439d-b111-8fd58d54ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "EVENTS_CSV_PATH = os.path.join(PATH_TO_OUTPUT_DIR, \"mimiciv.csv\")\n",
    "TEMP_CSV_PATH = os.path.join(PATH_TO_OUTPUT_DIR, \"mimiciv_final_fixed.csv\")\n",
    "\n",
    "OMOP_BIRTH_CODE = 'SNOMED/3950001'\n",
    "OMOP_DEATH_CODE = 'SNOMED/419620001'\n",
    "DEFAULT_PREFIX = \"MIMIC\"\n",
    "\n",
    "logger.info(f\" {EVENTS_CSV_PATH} ...\")\n",
    "\n",
    "try:\n",
    "    with open(EVENTS_CSV_PATH, 'r') as f:\n",
    "        total_lines = sum(1 for line in f)\n",
    "except Exception as e:\n",
    "    logger.error(f\": {e}\")\n",
    "    total_lines = 0\n",
    "\n",
    "try:\n",
    "    with open(EVENTS_CSV_PATH, 'r') as fin, open(TEMP_CSV_PATH, 'w', newline='') as fout:\n",
    "        reader = csv.reader(fin)\n",
    "        writer = csv.writer(fout)\n",
    "        \n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        code_col_idx = header.index('code')\n",
    "        \n",
    "        for row in tqdm(reader, total=total_lines - 1 if total_lines > 0 else None, desc=\"\"):\n",
    "            if len(row) > code_col_idx:\n",
    "                code_value = str(row[code_col_idx]).strip()\n",
    "                \n",
    "                if 'MEDS_BIRTH' in code_value: \n",
    "                    code_value = OMOP_BIRTH_CODE \n",
    "                elif 'MEDS_DEATH' in code_value:\n",
    "                     code_value = OMOP_DEATH_CODE \n",
    "                        \n",
    "                if code_value.startswith(\"['\") and code_value.endswith(\"']\"): \n",
    "                    code_value = code_value[2:-2]\n",
    "\n",
    "                is_standard_omop = (code_value.count('/') == 1 and '//' not in code_value)\n",
    "                \n",
    "                if not is_standard_omop:\n",
    "                    code_value = f\"{DEFAULT_PREFIX}/{code_value}\"\n",
    "                \n",
    "                row[code_col_idx] = code_value\n",
    "            \n",
    "            writer.writerow(row)\n",
    "\n",
    "    os.replace(TEMP_CSV_PATH, EVENTS_CSV_PATH)\n",
    "    logger.success(f\" mimiciv.csv ！\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\": {e}\")\n",
    "    if os.path.exists(TEMP_CSV_PATH):\n",
    "        os.remove(TEMP_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6688ca-9da9-4aa1-8b8b-691b060902b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nat_start_times_in_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    CSV，\n",
    "    'start'NaT， 'code' == 'SNOMED/3950001' 'start'。\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): CSV。\n",
    "    \"\"\"\n",
    "    \n",
    "    directory_path = os.path.expanduser(directory_path)\n",
    "    \n",
    "    logger.info(f\": {directory_path}\")\n",
    "\n",
    "    try:\n",
    "        all_csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "        if not all_csv_files:\n",
    "            logger.warning(f\" '{directory_path}'  .csv 。\")\n",
    "            return\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"： '{directory_path}'。。\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\" {len(all_csv_files)} CSV。\")\n",
    "    \n",
    "    files_processed = 0\n",
    "    files_with_changes = 0\n",
    "\n",
    "    for filename in tqdm(all_csv_files, desc=\"CSV\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, parse_dates=['start'])\n",
    "\n",
    "            birth_event = df[df['code'] == 'SNOMED/3950001']\n",
    "            \n",
    "            nat_rows = df['start'].isna()\n",
    "            \n",
    "            if not birth_event.empty and nat_rows.any():\n",
    "                birth_time = birth_event['start'].iloc[0]\n",
    "                \n",
    "                df.loc[nat_rows, 'start'] = birth_time\n",
    "                \n",
    "                df.to_csv(file_path, index=False)\n",
    "                \n",
    "                logger.success(f\" '{filename}':  {nat_rows.sum()} NaT。\")\n",
    "                files_with_changes += 1\n",
    "            \n",
    "            files_processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\" '{filename}' : {e}\")\n",
    "            \n",
    "    logger.info(\"---  ---\")\n",
    "    logger.info(f\" {files_processed} 。\")\n",
    "    logger.info(f\" {files_with_changes} 。\")\n",
    "\n",
    "fix_nat_start_times_in_directory(PATH_TO_OUTPUT_DIR)\n",
    "\n",
    "logger.success(\"！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e3075-3ebe-4c41-8fb5-27287a1207f1",
   "metadata": {},
   "source": [
    "## Step 3.1: Missing rate thredshold for Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b2984-775b-4fc1-bb09-8c93047eaf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_THRESHOLD = 0.9\n",
    "PROTECTED_CODES = {\"SNOMED/3950001\", \"SNOMED/419620001\"}  # /\n",
    "ICU_ADMIT_PREFIX = \"MIMIC/ICU_ADMISSION\"\n",
    "ICU_DISCHARGE_PREFIX = \"MIMIC/ICU_DISCHARGE\"\n",
    "\n",
    "PAT_COL = \"patient_id\"\n",
    "TIME_COL = \"start\"\n",
    "CODE_COL = \"code\"\n",
    "\n",
    "LOOKBACK_HOURS = 24  # ：ICU24h\n",
    "LOOKAHEAD_HOURS = 24  # ：ICU24h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486e905-31d8-4c45-bb0b-dbcc1e88f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _count_lines_fast(path: str, has_header: bool = True) -> int:\n",
    "    total = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            total += chunk.count(b\"\\n\")\n",
    "    if has_header and total > 0:\n",
    "        total -= 1\n",
    "    return max(total, 0)\n",
    "\n",
    "def _is_protected(code: str) -> bool:\n",
    "    return (\n",
    "        code in PROTECTED_CODES\n",
    "        or code.startswith(ICU_ADMIT_PREFIX)\n",
    "        or code.startswith(ICU_DISCHARGE_PREFIX)\n",
    "    )\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def first_pass_index_patients(\n",
    "    csv_path: str,\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 1_000_000,\n",
    "    encoding: str | None = None,\n",
    ") -> dict[str, dict[str, list[pd.Timestamp]]]:\n",
    "    \"\"\"\n",
    "     ICU //（，）。\n",
    "     .iat/.iloc ，，。\n",
    "    \"\"\"\n",
    "    info = defaultdict(lambda: {\"admits\": [], \"discharges\": [], \"deaths\": []})\n",
    "\n",
    "    total_rows = _count_lines_fast(csv_path, has_header=True)\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=\"Indexing ICU markers\") as pbar:\n",
    "        for chunk in pd.read_csv(\n",
    "            csv_path,\n",
    "            chunksize=chunksize,\n",
    "            sep=sep,\n",
    "            encoding=encoding,\n",
    "            usecols=[PAT_COL, TIME_COL, CODE_COL],\n",
    "            dtype={PAT_COL: \"string\", CODE_COL: \"string\"},\n",
    "            parse_dates=[TIME_COL],\n",
    "            infer_datetime_format=True,\n",
    "        ):\n",
    "            pid  = chunk[PAT_COL].astype(\"string\")\n",
    "            t    = chunk[TIME_COL]  # datetime64[ns]\n",
    "            code = chunk[CODE_COL].astype(\"string\")\n",
    "\n",
    "            m_admit = code.str.startswith(ICU_ADMIT_PREFIX, na=False)\n",
    "            m_dis   = code.str.startswith(ICU_DISCHARGE_PREFIX, na=False)\n",
    "            m_death = code.eq(\"SNOMED/419620001\")\n",
    "\n",
    "            def _accumulate(mask, key: str):\n",
    "                if not mask.any():\n",
    "                    return\n",
    "                sub = pd.DataFrame({PAT_COL: pid[mask].to_numpy(), TIME_COL: t[mask].to_numpy()})\n",
    "                grouped = sub.groupby(PAT_COL, sort=False)[TIME_COL].agg(list)\n",
    "                for p, ts_list in grouped.items():\n",
    "                    if ts_list:\n",
    "                        info[str(p)][key].extend([ts for ts in ts_list if pd.notna(ts)])\n",
    "\n",
    "            _accumulate(m_admit, \"admits\")\n",
    "            _accumulate(m_dis,   \"discharges\")\n",
    "            _accumulate(m_death, \"deaths\")\n",
    "\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    for p, d in info.items():\n",
    "        d[\"admits\"]     = sorted(set(d[\"admits\"]))\n",
    "        d[\"discharges\"] = sorted(set(d[\"discharges\"]))\n",
    "        d[\"deaths\"]     = sorted(set(d[\"deaths\"]))\n",
    "    return info\n",
    "\n",
    "\n",
    "\n",
    "def build_blocks_and_select_patients(\n",
    "    icu_info: Dict[str, Dict[str, List[pd.Timestamp]]],\n",
    "    drop_if_any_short_block: bool = True,\n",
    "    min_duration_hours: int = 24\n",
    ") -> Tuple[Dict[str, List[Tuple[pd.Timestamp, pd.Timestamp | None]]],\n",
    "           Dict[str, List[Tuple[pd.Timestamp, pd.Timestamp]]],\n",
    "           set, set]:\n",
    "    \"\"\"\n",
    "    ：\n",
    "      blocks[pid] = [(admit, end), ...]， end = min(/, )，None \n",
    "      pre_windows[pid] = [(admit-24h, admit), ...]\n",
    "      keep_patients: \n",
    "      drop_patients: \n",
    "    ：\n",
    "      1)  ICU  -> \n",
    "      2)  (admit -> /) < 24 ，（ drop_if_any_short_block ）\n",
    "    \"\"\"\n",
    "    lookback = pd.Timedelta(hours=LOOKBACK_HOURS)\n",
    "    min_dur  = pd.Timedelta(hours=min_duration_hours)\n",
    "\n",
    "    blocks = {}\n",
    "    pre_windows = {}\n",
    "    keep_patients, drop_patients = set(), set()\n",
    "\n",
    "    for pid, d in icu_info.items():\n",
    "        admits = d[\"admits\"]\n",
    "        if not admits:\n",
    "            drop_patients.add(pid)\n",
    "            continue\n",
    "\n",
    "        discharges = d[\"discharges\"]\n",
    "        deaths     = d[\"deaths\"]\n",
    "\n",
    "        segs = []\n",
    "        for i, a in enumerate(admits):\n",
    "            next_admit = admits[i+1] if i+1 < len(admits) else pd.Timestamp.max\n",
    "            closers = []\n",
    "            death_after = [dt for dt in deaths if dt > a]\n",
    "            if death_after:\n",
    "                closers.append(min(death_after))\n",
    "            dis_after = [dt for dt in discharges if dt > a]\n",
    "            if dis_after:\n",
    "                closers.append(min(dis_after))\n",
    "            closers.append(next_admit)\n",
    "            end = min(closers) if closers else pd.Timestamp.max\n",
    "\n",
    "            segs.append((a, end))\n",
    "\n",
    "        any_short = any(( (end - a) < min_dur ) for a, end in segs if end != pd.Timestamp.max)\n",
    "        if drop_if_any_short_block and any_short:\n",
    "            drop_patients.add(pid)\n",
    "            continue\n",
    "\n",
    "        keep_patients.add(pid)\n",
    "        blocks[pid] = segs\n",
    "        pre_windows[pid] = [(a - lookback, a) for a, _ in segs]\n",
    "\n",
    "    return blocks, pre_windows, keep_patients, drop_patients\n",
    "\n",
    "def build_codes_to_keep_by_row_fraction_with_progress(\n",
    "    csv_path: str,\n",
    "    out_dir: str,\n",
    "    threshold: float = DEFAULT_THRESHOLD,\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 1_000_000,\n",
    "    encoding: str | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "     codes_to_keep：【ICU  LOOKAHEAD_HOURS 】，\n",
    "    （）。\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    icu_info = first_pass_index_patients(csv_path, sep=sep, chunksize=chunksize, encoding=encoding)\n",
    "\n",
    "    blocks, _pre_windows_unused, keep_patients, drop_patients = build_blocks_and_select_patients(icu_info)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"kept_patients.json\"), \"w\") as f:\n",
    "        json.dump(sorted(list(keep_patients)), f)\n",
    "    with open(os.path.join(out_dir, \"dropped_patients.json\"), \"w\") as f:\n",
    "        json.dump(sorted(list(drop_patients)), f)\n",
    "\n",
    "    obs_delta = pd.Timedelta(hours=LOOKAHEAD_HOURS)\n",
    "    post_windows = {}\n",
    "    for pid, segs in blocks.items():\n",
    "        wins = []\n",
    "        for (a, end) in segs:\n",
    "            wins.append((a, min(a + obs_delta, end)))\n",
    "        post_windows[pid] = wins\n",
    "\n",
    "    total_rows_considered = 0\n",
    "    code_counts = Counter()\n",
    "    total_rows = _count_lines_fast(csv_path, has_header=True)\n",
    "\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=f\"Counting (post {LOOKAHEAD_HOURS}h)\") as pbar:\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, sep=sep, encoding=encoding,\n",
    "                                 usecols=[PAT_COL, TIME_COL, CODE_COL]):\n",
    "            pid  = chunk[PAT_COL].astype(str)\n",
    "            t    = pd.to_datetime(chunk[TIME_COL], errors=\"coerce\")\n",
    "            code = chunk[CODE_COL].astype(str)\n",
    "\n",
    "            m_keep_pid = pid.isin(keep_patients)\n",
    "            if not m_keep_pid.any():\n",
    "                pbar.update(len(chunk))\n",
    "                continue\n",
    "\n",
    "            sub  = chunk.loc[m_keep_pid].copy()\n",
    "            pid  = pid.loc[m_keep_pid]\n",
    "            t    = t.loc[m_keep_pid]\n",
    "            code = code.loc[m_keep_pid]\n",
    "\n",
    "            in_post = []\n",
    "            for idx in sub.index:\n",
    "                p = pid.at[idx]; ti = t.at[idx]\n",
    "                if pd.isna(ti):\n",
    "                    in_post.append(False)\n",
    "                    continue\n",
    "                hit = False\n",
    "                for lo, hi in post_windows.get(p, []):\n",
    "                    if ti >= lo and ti < hi:\n",
    "                        hit = True\n",
    "                        break\n",
    "                in_post.append(hit)\n",
    "            in_post = pd.Series(in_post, index=sub.index)\n",
    "\n",
    "            c = code.loc[in_post].astype(str)\n",
    "            m_not_prot = ~c.apply(_is_protected)\n",
    "            c = c.loc[m_not_prot]\n",
    "\n",
    "            total_rows_considered += len(c)\n",
    "            vc = c.value_counts()\n",
    "            for k, v in vc.items():\n",
    "                code_counts[k] += int(v)\n",
    "\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    denom = max(total_rows_considered, 1)\n",
    "    rows = []\n",
    "    for cd, n in code_counts.items():\n",
    "        row_fraction = n / denom\n",
    "        missing_rate = 1.0 - row_fraction\n",
    "        keep = (missing_rate < threshold)\n",
    "        rows.append({\n",
    "            \"code\": cd,\n",
    "            \"n_rows\": n,\n",
    "            \"total_rows\": denom,\n",
    "            \"row_fraction\": row_fraction,\n",
    "            \"missing_rate\": missing_rate,\n",
    "            \"is_protected\": _is_protected(cd),\n",
    "            \"keep\": keep,\n",
    "        })\n",
    "    stats_df = pd.DataFrame(rows).sort_values(by=[\"keep\", \"row_fraction\"], ascending=[False, False])\n",
    "\n",
    "    if not stats_df.empty:\n",
    "        prot_mask = stats_df[\"code\"].apply(_is_protected)\n",
    "        stats_df.loc[prot_mask, \"keep\"] = True\n",
    "\n",
    "    keep_csv = os.path.join(out_dir, \"codes_to_keep.csv\")\n",
    "    stats_df.to_csv(keep_csv, index=False)\n",
    "\n",
    "    keep_list = set(stats_df.loc[stats_df[\"keep\"], \"code\"].astype(str).tolist())\n",
    "    keep_list |= PROTECTED_CODES  # _is_protected\n",
    "    with open(os.path.join(out_dir, \"codes_to_keep.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted(list(keep_list)), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return keep_csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_codes_to_keep_on_csv_with_progress(\n",
    "    csv_path: str,\n",
    "    keep_file: str,  # codes_to_keep.csv  .json\n",
    "    out_path: str,\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 1_000_000,\n",
    "    encoding: str | None = None,\n",
    ") -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    ：\n",
    "      -  kept_patients（ kept_patients.json）\n",
    "      - code  keep_set \n",
    "      -  ICU block(admit -> end):  (admit+24h, end) ；**** /（）\n",
    "    \"\"\"\n",
    "    out_dir = os.path.dirname(out_path) or \".\"\n",
    "    kept_pat_json = os.path.join(out_dir, \"kept_patients.json\")\n",
    "    if not os.path.exists(kept_pat_json):\n",
    "        raise FileNotFoundError(\" kept_patients.json，（build_codes_to_keep_...）\")\n",
    "\n",
    "    if keep_file.endswith(\".json\"):\n",
    "        keep_set = set(json.load(open(keep_file, \"r\", encoding=\"utf-8\")))\n",
    "    else:\n",
    "        kdf = pd.read_csv(keep_file, usecols=[\"code\", \"keep\"])\n",
    "        keep_set = set(kdf.loc[kdf[\"keep\"], \"code\"].astype(str).tolist())\n",
    "    keep_set |= PROTECTED_CODES | {ICU_ADMIT_PREFIX, ICU_DISCHARGE_PREFIX}\n",
    "\n",
    "    kept_patients = set(json.load(open(kept_pat_json, \"r\")))\n",
    "    icu_info = first_pass_index_patients(csv_path, sep=sep, chunksize=chunksize, encoding=encoding)\n",
    "    blocks, _, _, _ = build_blocks_and_select_patients(icu_info)\n",
    "\n",
    "    drop_intervals = {}\n",
    "    for pid, segs in blocks.items():\n",
    "        lst = []\n",
    "        for a, end in segs:\n",
    "            lo = a + pd.Timedelta(hours=LOOKAHEAD_HOURS)\n",
    "            hi = end\n",
    "            lst.append((lo, hi))\n",
    "        drop_intervals[pid] = lst\n",
    "\n",
    "    total_rows = _count_lines_fast(csv_path, has_header=True)\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    first = True\n",
    "    written = 0\n",
    "\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=\"Filtering\") as pbar:\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=chunksize, sep=sep, encoding=encoding):\n",
    "            if CODE_COL not in chunk.columns or PAT_COL not in chunk.columns or TIME_COL not in chunk.columns:\n",
    "                raise KeyError(f\"CSV ：{[PAT_COL, TIME_COL, CODE_COL]}\")\n",
    "            pid  = chunk[PAT_COL].astype(str)\n",
    "            t    = pd.to_datetime(chunk[TIME_COL], errors=\"coerce\")\n",
    "            code = chunk[CODE_COL].astype(str)\n",
    "\n",
    "            m_keep_pid = pid.isin(kept_patients)\n",
    "            if not m_keep_pid.any():\n",
    "                pbar.update(len(chunk))\n",
    "                continue\n",
    "            sub = chunk.loc[m_keep_pid].copy()\n",
    "            pid = pid.loc[m_keep_pid]; t = t.loc[m_keep_pid]; code = code.loc[m_keep_pid]\n",
    "\n",
    "            m_code = code.apply(lambda c: (c in keep_set) or _is_protected(c))\n",
    "            sub = sub.loc[m_code]; pid = pid.loc[m_code]; t = t.loc[m_code]; code = code.loc[m_code]\n",
    "            if sub.empty:\n",
    "                pbar.update(len(chunk))\n",
    "                continue\n",
    "\n",
    "            rows_keep = []\n",
    "            for idx in sub.index:\n",
    "                p = pid.at[idx]; ti = t.at[idx]; ci = code.at[idx]\n",
    "                if pd.isna(ti):\n",
    "                    rows_keep.append(True)\n",
    "                    continue\n",
    "                if _is_protected(ci):\n",
    "                    rows_keep.append(True)\n",
    "                    continue\n",
    "                drop = False\n",
    "                for (lo, hi) in drop_intervals.get(p, []):\n",
    "                    if ti > lo and ti < hi:\n",
    "                        drop = True\n",
    "                        break\n",
    "                rows_keep.append(not drop)\n",
    "\n",
    "            out_chunk = sub.loc[pd.Series(rows_keep, index=sub.index)]\n",
    "            out_chunk.to_csv(out_path, mode=\"w\" if first else \"a\", header=first, index=False)\n",
    "            first = False\n",
    "            written += len(out_chunk)\n",
    "\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    return out_path, written\n",
    "\n",
    "def build_keep_only(\n",
    "    src_csv: str,\n",
    "    out_dir: str,\n",
    "    threshold: float = DEFAULT_THRESHOLD,\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 1_000_000,\n",
    "    encoding: str | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    “+ keep  +  kept_patients.json / dropped_patients.json”，。\n",
    "    \"\"\"\n",
    "    return build_codes_to_keep_by_row_fraction_with_progress(\n",
    "        src_csv, out_dir, threshold, sep, chunksize, encoding\n",
    "    )\n",
    "\n",
    "def filter_only(\n",
    "    src_csv: str,\n",
    "    out_dir: str,\n",
    "    keep_file: str,  # build_keep_only  codes_to_keep.csv/json\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 1_000_000,\n",
    "    encoding: str | None = None,\n",
    ") -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    ， keep_file + kept_patients.json。\n",
    "    ：out_dir/filtered.csv\n",
    "    \"\"\"\n",
    "    filtered_csv = os.path.join(out_dir, \"filtered.csv\")\n",
    "    return apply_codes_to_keep_on_csv_with_progress(\n",
    "        src_csv, keep_file, filtered_csv, sep, chunksize, encoding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90738a85-62cd-48f0-929b-47e0f11606e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keep_csv = build_keep_only(\n",
    "    src_csv=\"/root/autodl-tmp/femr/train/mimiciv.csv\",\n",
    "    out_dir=\"/root/autodl-tmp/femr_filtered/train\",  \n",
    "    threshold=0.9,\n",
    "    sep=\",\",\n",
    "    chunksize=1_000_000,\n",
    ")\n",
    "print(\"codes_to_keep ->\", keep_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb732d-e538-4991-85a7-2cd0da30e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as filter_icu_24h_then_stats.py\n",
    "import os, json\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "PAT_COL  = \"patient_id\"\n",
    "TIME_COL = \"start\"  # （CSV  pandas ）\n",
    "CODE_COL = \"code\"  # code\n",
    "\n",
    "PROTECTED_CODES = {\"SNOMED/3950001\", \"SNOMED/419620001\"}  # /\n",
    "ICU_ADMIT_PREFIX     = \"MIMIC/ICU_ADMISSION\"\n",
    "ICU_DISCHARGE_PREFIX = \"MIMIC/ICU_DISCHARGE\"\n",
    "\n",
    "WINDOW_HOURS = 24  # 24h\n",
    "\n",
    "def _count_lines_fast(path: str, has_header: bool = True) -> int:\n",
    "    total = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            total += chunk.count(b\"\\n\")\n",
    "    if has_header and total > 0:\n",
    "        total -= 1\n",
    "    return max(total, 0)\n",
    "\n",
    "def _is_birth(c: pd.Series) -> pd.Series:\n",
    "    return c == \"SNOMED/3950001\"\n",
    "\n",
    "def _is_death(c: pd.Series) -> pd.Series:\n",
    "    return c == \"SNOMED/419620001\"\n",
    "\n",
    "def _is_icu_admit(c: pd.Series) -> pd.Series:\n",
    "    return c.str.startswith(ICU_ADMIT_PREFIX, na=False)\n",
    "\n",
    "def _is_icu_discharge(c: pd.Series) -> pd.Series:\n",
    "    return c.str.startswith(ICU_DISCHARGE_PREFIX, na=False)\n",
    "\n",
    "def _is_protected_any(c: pd.Series) -> pd.Series:\n",
    "    return _is_birth(c) | _is_death(c) | _is_icu_admit(c) | _is_icu_discharge(c)\n",
    "\n",
    "def _is_allowed_after_24h(c: pd.Series) -> pd.Series:\n",
    "    return _is_death(c) | _is_icu_discharge(c)\n",
    "\n",
    "def prefilter_drop_mimic_except_icu(\n",
    "    src_csv: str,\n",
    "    out_csv: str,\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 2_000_000,\n",
    "    encoding: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "     'MIMIC/' ，**** ICU /；\n",
    "    。\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    import csv as _csv\n",
    "    with open(src_csv, \"r\", encoding=encoding or \"utf-8\") as f:\n",
    "        original_cols = next(_csv.reader(f))\n",
    "\n",
    "    total_rows = _count_lines_fast(src_csv, has_header=True)\n",
    "    first = True\n",
    "    written = 0\n",
    "\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=\"Pre-filter: drop MIMIC/* (keep ICU markers)\") as pbar:\n",
    "        reader = pd.read_csv(src_csv, sep=sep, chunksize=chunksize, encoding=encoding)\n",
    "        for chunk in reader:\n",
    "            code = chunk[CODE_COL].astype(\"string\")\n",
    "            is_mimic   = code.str.startswith(\"MIMIC/\", na=False)\n",
    "            keep_icu   = _is_icu_admit(code) | _is_icu_discharge(code)\n",
    "            to_drop    = is_mimic & (~keep_icu)\n",
    "\n",
    "            out_chunk = chunk.loc[~to_drop].reindex(columns=original_cols)\n",
    "            out_chunk.to_csv(out_csv, mode=\"w\" if first else \"a\", header=first, index=False)\n",
    "            first = False\n",
    "            written += len(out_chunk)\n",
    "            pbar.update(len(chunk))\n",
    "    return out_csv, written\n",
    "\n",
    "\n",
    "\n",
    "def index_icu_markers(csv_path: str, sep=\",\", chunksize=2_000_000, encoding=None):\n",
    "    info = defaultdict(lambda: {\"admits\": [], \"discharges\": [], \"deaths\": []})\n",
    "    total_rows = _count_lines_fast(csv_path, has_header=True)\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=\"Indexing ICU markers\") as pbar:\n",
    "        reader = pd.read_csv(\n",
    "            csv_path, sep=sep, chunksize=chunksize, encoding=encoding,\n",
    "            usecols=[PAT_COL, TIME_COL, CODE_COL],\n",
    "            dtype={PAT_COL:\"string\", CODE_COL:\"string\"},\n",
    "            parse_dates=[TIME_COL],\n",
    "        )\n",
    "        for chunk in reader:\n",
    "            pid  = chunk[PAT_COL].astype(\"string\")\n",
    "            code = chunk[CODE_COL].astype(\"string\")\n",
    "\n",
    "            m_admit = _is_icu_admit(code)\n",
    "            m_dis   = _is_icu_discharge(code)\n",
    "            m_death = _is_death(code)\n",
    "\n",
    "            def _acc(mask, key: str):\n",
    "                if not mask.any():\n",
    "                    return\n",
    "                sub = chunk.loc[mask, [PAT_COL, TIME_COL]].copy()\n",
    "                if not np.issubdtype(sub[TIME_COL].dtype, np.datetime64):\n",
    "                    sub[TIME_COL] = pd.to_datetime(sub[TIME_COL], errors=\"coerce\")\n",
    "                sub = sub.dropna(subset=[TIME_COL])\n",
    "                if sub.empty: return\n",
    "                g = sub.groupby(PAT_COL, sort=False)[TIME_COL].agg(list)\n",
    "                for p, ts_list in g.items():\n",
    "                    info[str(p)][key].extend(ts_list)\n",
    "\n",
    "            _acc(m_admit, \"admits\")\n",
    "            _acc(m_dis,   \"discharges\")\n",
    "            _acc(m_death, \"deaths\")\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    for p, d in info.items():\n",
    "        d[\"admits\"]     = sorted(set(d[\"admits\"]))\n",
    "        d[\"discharges\"] = sorted(set(d[\"discharges\"]))\n",
    "        d[\"deaths\"]     = sorted(set(d[\"deaths\"]))\n",
    "    return info\n",
    "\n",
    "def save_icu_index(icu_info: dict, path: str):\n",
    "    \"\"\" ICU  JSON（ISO ）， .gz 。\"\"\"\n",
    "    rec = {\n",
    "        p: {\n",
    "            \"admits\":     [ts.isoformat() for ts in d[\"admits\"]],\n",
    "            \"discharges\": [ts.isoformat() for ts in d[\"discharges\"]],\n",
    "            \"deaths\":     [ts.isoformat() for ts in d[\"deaths\"]],\n",
    "        } for p, d in icu_info.items()\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    if path.endswith(\".gz\"):\n",
    "        import gzip\n",
    "        with gzip.open(path, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rec, f)\n",
    "    else:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rec, f)\n",
    "    return path\n",
    "\n",
    "def load_icu_index(path: str) -> dict:\n",
    "    \"\"\" Step 1  ICU  JSON（ .gz）， Timestamp。\"\"\"\n",
    "    if path.endswith(\".gz\"):\n",
    "        import gzip\n",
    "        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "            rec = json.load(f)\n",
    "    else:\n",
    "        rec = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    out = {}\n",
    "    for p, d in rec.items():\n",
    "        out[p] = {\n",
    "            \"admits\":     [pd.Timestamp(x) for x in d[\"admits\"]],\n",
    "            \"discharges\": [pd.Timestamp(x) for x in d[\"discharges\"]],\n",
    "            \"deaths\":     [pd.Timestamp(x) for x in d[\"deaths\"]],\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def build_stays(icu_info: Dict[str, Dict[str, List[pd.Timestamp]]], window_hours=24):\n",
    "    kept_stays, dropped_stays = {}, {}\n",
    "    delta = pd.Timedelta(hours=window_hours)\n",
    "    for pid, d in icu_info.items():\n",
    "        admits = d[\"admits\"]; \n",
    "        if not admits: continue\n",
    "        dis, deaths = d[\"discharges\"], d[\"deaths\"]\n",
    "        kept, dropped = [], []\n",
    "        for i, A in enumerate(admits):\n",
    "            next_A = admits[i+1] if i+1 < len(admits) else pd.Timestamp.max\n",
    "            dis_after   = [x for x in dis    if x > A]\n",
    "            death_after = [x for x in deaths if x > A]\n",
    "            candidates = [next_A]\n",
    "            if dis_after:   candidates.append(min(dis_after))\n",
    "            if death_after: candidates.append(min(death_after))\n",
    "            E = min(candidates) if candidates else pd.Timestamp.max\n",
    "            too_short = (E - A) < delta\n",
    "            death_within_24h = any((x - A) <= delta for x in death_after)\n",
    "            if too_short or death_within_24h:\n",
    "                dropped.append((A, E))\n",
    "            else:\n",
    "                cutoff = min(A + delta, E)\n",
    "                kept.append((A, cutoff, E))\n",
    "        if kept:   kept_stays[pid] = kept\n",
    "        if dropped:dropped_stays[pid] = dropped\n",
    "    return kept_stays, dropped_stays\n",
    "\n",
    "def save_stays(kept_stays: dict, dropped_stays: dict, kept_path: str, dropped_path: str):\n",
    "    def _ser(obj):\n",
    "        return {p: [(str(a), str(c), str(e)) for (a, c, e) in lst] for p, lst in obj.items()}\n",
    "    os.makedirs(os.path.dirname(kept_path) or \".\", exist_ok=True)\n",
    "    json.dump(_ser(kept_stays), open(kept_path, \"w\"), ensure_ascii=False)\n",
    "    json.dump({p: [(str(a), str(e)) for (a, e) in lst] for p, lst in dropped_stays.items()},\n",
    "              open(dropped_path, \"w\"), ensure_ascii=False)\n",
    "    return kept_path, dropped_path\n",
    "\n",
    "def load_stays(kept_path: str, dropped_path: str):\n",
    "    ks = json.load(open(kept_path, \"r\")); ds = json.load(open(dropped_path, \"r\"))\n",
    "    kept = {p: [(pd.Timestamp(a), pd.Timestamp(c), pd.Timestamp(e)) for (a, c, e) in lst] for p, lst in ks.items()}\n",
    "    drop = {p: [(pd.Timestamp(a), pd.Timestamp(e)) for (a, e) in lst] for p, lst in ds.items()}\n",
    "    return kept, drop\n",
    "\n",
    "\n",
    "\n",
    "def _membership_via_searchsorted(times: np.ndarray, starts: np.ndarray, ends: np.ndarray, inclusive_end=False):\n",
    "    \"\"\"\n",
    "     [starts[i], ends[i])， times 。\n",
    "    ：(in_interval, idx)  idx （<0 ）。\n",
    "    \"\"\"\n",
    "    if len(starts) == 0:\n",
    "        return np.zeros_like(times, dtype=bool), np.full(times.shape, -1, dtype=int)\n",
    "    idx = np.searchsorted(starts, times, side=\"right\") - 1  # -1 =  start<=t\n",
    "    valid = idx >= 0\n",
    "    e_sel = np.empty_like(times)\n",
    "    e_sel[~valid] = np.datetime64(\"NaT\")\n",
    "    e_sel[valid]  = ends[idx[valid]]\n",
    "    if inclusive_end:\n",
    "        in_itv = valid & (times <= e_sel)\n",
    "    else:\n",
    "        in_itv = valid & (times < e_sel)\n",
    "    return in_itv, idx\n",
    "\n",
    "def filter_csv_by_icu_rules(\n",
    "    src_csv: str,\n",
    "    out_csv: str,\n",
    "    kept_stays: Dict[str, List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp]]],\n",
    "    dropped_stays: Dict[str, List[Tuple[pd.Timestamp, pd.Timestamp]]],\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 2_000_000,\n",
    "    encoding: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    ：\n",
    "      1) stay ：  code（//ICU /）\n",
    "      2)  stay：（ code）\n",
    "      3)  stay：\n",
    "          - [admit, admit+24h) ：\n",
    "          - [admit+24h, end] ： {ICU , }\n",
    "    \"\"\"\n",
    "    total_rows = _count_lines_fast(src_csv, has_header=True)\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    import csv as _csv\n",
    "    with open(src_csv, \"r\", encoding=encoding or \"utf-8\") as f:\n",
    "        original_cols = next(_csv.reader(f))\n",
    "\n",
    "    first = True\n",
    "    written = 0\n",
    "\n",
    "    kept_pids    = set(kept_stays.keys())\n",
    "    dropped_pids = set(dropped_stays.keys())\n",
    "\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=\"Filtering by ICU rules\") as pbar:\n",
    "        reader = pd.read_csv(src_csv, sep=sep, chunksize=chunksize, encoding=encoding, parse_dates=[TIME_COL])\n",
    "        for chunk in reader:\n",
    "            if not np.issubdtype(chunk[TIME_COL].dtype, np.datetime64):\n",
    "                chunk[TIME_COL] = pd.to_datetime(chunk[TIME_COL], errors=\"coerce\")\n",
    "\n",
    "            pid  = chunk[PAT_COL].astype(\"string\")\n",
    "            time = chunk[TIME_COL].to_numpy(dtype=\"datetime64[ns]\", copy=False)\n",
    "            code = chunk[CODE_COL].astype(\"string\")\n",
    "\n",
    "            keep_mask = np.zeros(len(chunk), dtype=bool)\n",
    "\n",
    "            idx_all = np.arange(len(chunk))\n",
    "            m_kept_pid    = pid.isin(kept_pids).to_numpy()\n",
    "            m_dropped_pid = pid.isin(dropped_pids).to_numpy()\n",
    "            m_no_stay_pid = ~(m_kept_pid | m_dropped_pid)\n",
    "\n",
    "            if m_no_stay_pid.any():\n",
    "                sub = code[m_no_stay_pid]\n",
    "                keep_mask[m_no_stay_pid] = _is_protected_any(sub).to_numpy()\n",
    "\n",
    "            if m_dropped_pid.any():\n",
    "                ids = idx_all[m_dropped_pid]\n",
    "                for p in np.unique(pid.iloc[ids]):\n",
    "                    mask_p = (pid.iloc[ids] == p).to_numpy()\n",
    "                    ids_p  = ids[mask_p]\n",
    "                    times_p= time[ids_p]\n",
    "\n",
    "                    dlist = dropped_stays.get(str(p), [])\n",
    "                    d_starts = np.array([np.datetime64(a, \"ns\") for a, _ in dlist])\n",
    "                    d_ends   = np.array([np.datetime64(e, \"ns\") for _, e in dlist])\n",
    "                    in_dropped, _ = _membership_via_searchsorted(times_p, d_starts, d_ends, inclusive_end=True)\n",
    "\n",
    "                    keep_outside = _is_protected_any(code.iloc[ids_p]).to_numpy()\n",
    "                    keep_mask[ids_p] = (~in_dropped) & keep_outside\n",
    "\n",
    "            if m_kept_pid.any():\n",
    "                ids = idx_all[m_kept_pid]\n",
    "                for p in np.unique(pid.iloc[ids]):\n",
    "                    mask_p = (pid.iloc[ids] == p).to_numpy()\n",
    "                    ids_p  = ids[mask_p]\n",
    "                    times_p= time[ids_p]\n",
    "                    codes_p= code.iloc[ids_p]\n",
    "\n",
    "                    kept_list = kept_stays.get(str(p), [])\n",
    "                    k_starts = np.array([np.datetime64(a, \"ns\") for a, _, _ in kept_list])\n",
    "                    k_cuts   = np.array([np.datetime64(c, \"ns\") for _, c, _ in kept_list])\n",
    "                    k_ends   = np.array([np.datetime64(e, \"ns\") for _, _, e in kept_list])\n",
    "\n",
    "                    in_kept, idxk = _membership_via_searchsorted(times_p, k_starts, k_ends, inclusive_end=False)\n",
    "\n",
    "                    validk = idxk >= 0\n",
    "                    cut_sel = np.empty_like(times_p)\n",
    "                    end_sel = np.empty_like(times_p)\n",
    "                    cut_sel[validk] = k_cuts[idxk[validk]]\n",
    "                    end_sel[validk] = k_ends[idxk[validk]]\n",
    "                    keep_0_24 = in_kept & (times_p < cut_sel)\n",
    "\n",
    "                    allow_after = _is_allowed_after_24h(codes_p).to_numpy()\n",
    "                    in_after = in_kept & (times_p >= cut_sel) & (times_p <= end_sel)\n",
    "                    keep_24_E = in_after & allow_after\n",
    "\n",
    "                    outside_kept = ~in_kept\n",
    "                    keep_outside = _is_protected_any(codes_p).to_numpy()\n",
    "\n",
    "                    keep_mask[ids_p] = keep_0_24 | keep_24_E | (outside_kept & keep_outside)\n",
    "\n",
    "            out_chunk = chunk.loc[keep_mask].reindex(columns=original_cols)\n",
    "            out_chunk.to_csv(out_csv, mode=\"w\" if first else \"a\", header=first, index=False)\n",
    "            first = False\n",
    "            written += len(out_chunk)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    return out_csv, written\n",
    "\n",
    "def stats_row_fraction(csv_path: str, out_csv: str, sep=\",\", chunksize=2_000_000, encoding=None, exclude_protected=True):\n",
    "    \"\"\"\n",
    "    （）：missing_rate(code) = 1 - n_rows(code)/total_rows\n",
    "    ：exclude_protected=True ， code /。\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    counts = Counter()\n",
    "    total_rows = _count_lines_fast(csv_path, has_header=True)\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=\"Counting codes (filtered CSV)\") as pbar:\n",
    "        reader = pd.read_csv(csv_path, sep=sep, chunksize=chunksize, encoding=encoding, usecols=[CODE_COL])\n",
    "        for chunk in reader:\n",
    "            c = chunk[CODE_COL].astype(\"string\")\n",
    "            if exclude_protected:\n",
    "                m = ~_is_protected_any(c)\n",
    "                c = c[m]\n",
    "                inc_total = int(m.sum())\n",
    "            else:\n",
    "                inc_total = len(c)\n",
    "            vc = c.value_counts()\n",
    "            for k, v in vc.items():\n",
    "                counts[str(k)] += int(v)\n",
    "            total += inc_total\n",
    "            pbar.update(len(chunk))\n",
    "    total = max(total, 1)\n",
    "    rows = []\n",
    "    for code, n in counts.items():\n",
    "        row_fraction = n / total\n",
    "        rows.append({\n",
    "            \"code\": code,\n",
    "            \"n_rows\": n,\n",
    "            \"total_rows\": total,\n",
    "            \"row_fraction\": row_fraction,\n",
    "            \"missing_rate\": 1.0 - row_fraction,\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values(by=[\"row_fraction\"], ascending=False)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    return out_csv\n",
    "\n",
    "def run_filter_then_stats(src_csv: str, out_dir: str, sep=\",\", chunksize=2_000_000):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    icu_info = index_icu_markers(src_csv, sep=sep, chunksize=chunksize)\n",
    "    kept_stays, dropped_stays = build_stays(icu_info, window_hours=WINDOW_HOURS)\n",
    "\n",
    "    filtered_csv = os.path.join(out_dir, \"filtered.csv\")\n",
    "    filter_csv_by_icu_rules(src_csv, filtered_csv, kept_stays, dropped_stays, sep=sep, chunksize=chunksize)\n",
    "\n",
    "    stats_csv = os.path.join(out_dir, \"code_row_fraction_stats.csv\")\n",
    "    stats_row_fraction(filtered_csv, stats_csv, sep=sep, chunksize=chunksize, exclude_protected=True)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"kept_stays.json\"), \"w\") as f:\n",
    "        json.dump({p: [(str(a), str(c), str(e)) for (a, c, e) in lst] for p, lst in kept_stays.items()}, f)\n",
    "    with open(os.path.join(out_dir, \"dropped_stays.json\"), \"w\") as f:\n",
    "        json.dump({p: [(str(a), str(e)) for (a, e) in lst] for p, lst in dropped_stays.items()}, f)\n",
    "\n",
    "    return filtered_csv, stats_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d05c17-7ecd-4ef2-90d4-814e0c4bff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_csv  = \"/root/autodl-tmp/femr/tuning/mimiciv.csv\"\n",
    "out_dir  = \"/root/autodl-tmp/femr_filtered/tuning\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "pre_csv = os.path.join(out_dir, \"pre_filtered.csv\")\n",
    "prefilter_drop_mimic_except_icu(src_csv, pre_csv, chunksize=2_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4bffc-4c7d-4707-9e62-5e01716ec156",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_csv  = \"/root/autodl-tmp/femr/tuning/mimiciv.csv\"\n",
    "out_dir  = \"/root/autodl-tmp/femr_filtered/tuning\"\n",
    "pre_csv = os.path.join(out_dir, \"pre_filtered.csv\")\n",
    "\n",
    "icu_index_path = os.path.join(out_dir, \"icu_index.json.gz\")\n",
    "icu_info = index_icu_markers(pre_csv, chunksize=2_000_000)\n",
    "save_icu_index(icu_info, icu_index_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3231a7-c450-448d-a893-818319d3e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_path    = os.path.join(out_dir, \"kept_stays.json\")\n",
    "dropped_path = os.path.join(out_dir, \"dropped_stays.json\")\n",
    "kept_stays, dropped_stays = build_stays(icu_info, window_hours=24)\n",
    "save_stays(kept_stays, dropped_stays, kept_path, dropped_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4e22f-d53e-44ea-895a-156663ba2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_csv = os.path.join(out_dir, \"filtered.csv\")\n",
    "filter_csv_by_icu_rules(pre_csv, filtered_csv, kept_stays, dropped_stays, chunksize=2_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a71b27-75bf-495b-866a-1346e3c2a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_csv = os.path.join(out_dir, \"code_row_fraction_stats.csv\")\n",
    "stats_row_fraction(filtered_csv, stats_csv, exclude_protected=True, chunksize=2_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b71f7-1df2-4d71-b9da-244105b0d39a",
   "metadata": {},
   "source": [
    "delete events by missing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59625d6c-7944-4321-8a5a-1e8e595f3484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "from typing import Set\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROTECTED_CODES = {\"SNOMED/3950001\", \"SNOMED/419620001\"}  # /\n",
    "ICU_ADMIT_PREFIX     = \"MIMIC/ICU_ADMISSION\"\n",
    "ICU_DISCHARGE_PREFIX = \"MIMIC/ICU_DISCHARGE\"\n",
    "\n",
    "def _count_lines_fast(path: str, has_header: bool = True) -> int:\n",
    "    total = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            total += chunk.count(b\"\\n\")\n",
    "    if has_header and total > 0:\n",
    "        total -= 1\n",
    "    return max(total, 0)\n",
    "\n",
    "def _load_top_codes(stats_csv: str, top_n: int = 144) -> Set[str]:\n",
    "    \"\"\" stats_csv  code （ row_fraction ）。\"\"\"\n",
    "    df = pd.read_csv(stats_csv, dtype={\"code\": \"string\"})\n",
    "    if \"row_fraction\" in df.columns:\n",
    "        df = df.sort_values(\"row_fraction\", ascending=False)\n",
    "    top = df[\"code\"].dropna().astype(str).head(top_n).tolist()\n",
    "    return set(top)\n",
    "\n",
    "def filter_to_top_codes(\n",
    "    filtered_csv: str,\n",
    "    stats_csv: str,\n",
    "    out_csv: str,\n",
    "    top_n: int = 144,\n",
    "    sep: str = \",\",\n",
    "    chunksize: int = 1_000_000,\n",
    "    encoding: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    ：stats  top_n  code +  code（//ICU/），。\n",
    "    ；；。\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    keep_codes = _load_top_codes(stats_csv, top_n=top_n)\n",
    "    keep_codes |= PROTECTED_CODES  # /\n",
    "\n",
    "    with open(filtered_csv, \"r\", encoding=encoding or \"utf-8\") as f:\n",
    "        original_cols = next(csv.reader(f))\n",
    "\n",
    "    total_rows = _count_lines_fast(filtered_csv, has_header=True)\n",
    "    first = True\n",
    "    written = 0\n",
    "\n",
    "    desc = f\"Filtering to top {top_n} codes (+protected)\"\n",
    "    with tqdm(total=total_rows or None, unit=\"row\", desc=desc) as pbar:\n",
    "        reader = pd.read_csv(\n",
    "            filtered_csv, sep=sep, chunksize=chunksize, encoding=encoding,\n",
    "            dtype={\"code\": \"string\"}  # 'SNOMED/2.26E+14'\n",
    "        )\n",
    "        for chunk in reader:\n",
    "            codes = chunk[\"code\"].astype(\"string\")\n",
    "            mask = (\n",
    "                codes.isin(keep_codes)\n",
    "                | codes.str.startswith(ICU_ADMIT_PREFIX, na=False)\n",
    "                | codes.str.startswith(ICU_DISCHARGE_PREFIX, na=False)\n",
    "            )\n",
    "            out_chunk = chunk.loc[mask].reindex(columns=original_cols)\n",
    "            out_chunk.to_csv(out_csv, mode=\"w\" if first else \"a\",\n",
    "                             header=first, index=False)\n",
    "            first = False\n",
    "            written += len(out_chunk)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "    return out_csv, written\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0eb9a-3e98-4058-b15d-592fbecb1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/root/autodl-tmp/femr_filtered/held_out\"\n",
    "stats_csv = os.path.join('/root/autodl-tmp/femr_filtered/train', \"code_row_fraction_stats.csv\")\n",
    "src      = os.path.join(out_dir, \"filtered.csv\")  # filtered.csv\n",
    "dst      = os.path.join(out_dir, \"filtered_top144.csv\")\n",
    "\n",
    "filter_to_top_codes(\n",
    "    filtered_csv=src,\n",
    "    stats_csv=stats_csv,\n",
    "    out_csv=dst,\n",
    "    top_n=144,\n",
    "    sep=\",\",\n",
    "    chunksize=1_000_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a3be1-34c3-427a-929f-382f14a3dc53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 4: Split CSV into multiple shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6379e13-65c1-4dd8-aab7-5693763543f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_OUTPUT_DIR = \"/root/autodl-tmp/femr_filtered/held_out\"\n",
    "EVENTS_CSV_PATH = os.path.join(PATH_TO_OUTPUT_DIR, \"filtered_top144.csv\")\n",
    "NUM_PROCESSES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30eee21-ae88-4f15-83e5-651015c0a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "outdir_main = Path(PATH_TO_OUTPUT_DIR) / \"shards\"\n",
    "\n",
    "if os.path.exists(outdir_main): shutil.rmtree(outdir_main)\n",
    "outdir_main.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_SPLITS = 30\n",
    "\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def split_for_multicore():\n",
    "    \"\"\" NUM_SPLITS ，\"\"\"\n",
    "    with open(EVENTS_CSV_PATH) as f:\n",
    "        total_lines = sum(1 for _ in f) - 1\n",
    "    rows_per_split = (total_lines // NUM_SPLITS) + 1\n",
    "    print(f\"[]  {total_lines},  {rows_per_split} \")\n",
    "\n",
    "    reader = pd.read_csv(EVENTS_CSV_PATH, chunksize=rows_per_split)\n",
    "    for idx, chunk in enumerate(reader):\n",
    "        shard_path = outdir_main / f\"shard_{idx:02d}.csv\"\n",
    "        chunk.to_csv(shard_path, index=False)\n",
    "        print(f\"[]  {shard_path} ({len(chunk)} )\")\n",
    "              \n",
    "split_for_multicore()\n",
    "print(\"\\n✅ ！\")\n",
    "print(f\": {outdir_main}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951d149-8bc1-47db-b176-b299fb06bb6f",
   "metadata": {},
   "source": [
    "# Step 5: Build FEMR databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed499d-ce26-47b7-9aeb-20ce39beeddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMR_INPUT_DIR_CLEAN = outdir_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc60a4b-097b-4ad2-9dd6-f1e3adf78566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "logger.info(\"---  3:  etl_simple_femr  ---\")\n",
    "python_executable_path = sys.executable\n",
    "path_to_bin = os.path.dirname(python_executable_path)\n",
    "path_to_etl_simple_femr = os.path.join(path_to_bin, \"etl_simple_femr\")\n",
    "\n",
    "if not os.path.exists(path_to_etl_simple_femr):\n",
    "    raise FileNotFoundError(f\" {path_to_etl_simple_femr}  'etl_simple_femr'。\")\n",
    "\n",
    "FEMR_EXTRACT_DIR = os.path.join(PATH_TO_OUTPUT_DIR, \"extract\")\n",
    "FEMR_LOGS_DIR = os.path.join(PATH_TO_OUTPUT_DIR, \"logs\")\n",
    "if os.path.exists(FEMR_EXTRACT_DIR): shutil.rmtree(FEMR_EXTRACT_DIR)\n",
    "if os.path.exists(FEMR_LOGS_DIR): shutil.rmtree(FEMR_LOGS_DIR)\n",
    "\n",
    "command = (\n",
    "    f\"{path_to_etl_simple_femr} {FEMR_INPUT_DIR_CLEAN} {FEMR_EXTRACT_DIR} {FEMR_LOGS_DIR} --num_threads {NUM_PROCESSES}\"\n",
    ")\n",
    "\n",
    "logger.info(f\": {command}\")\n",
    "exit_code = os.system(command)\n",
    "\n",
    "if exit_code == 0:\n",
    "    logger.success(\"---  3: FEMR ！---\")\n",
    "else:\n",
    "    logger.error(f\"---  3: etl_simple_femr ，: {exit_code} ---\")\n",
    "    raise ChildProcessError(\"etl_simple_femr failed. Check logs in \" + FEMR_LOGS_DIR + \" for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8b8b2-1b97-4c2d-b70c-3bb04f56347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMR_EXTRACT_DIR = os.path.join(PATH_TO_OUTPUT_DIR, \"extract\")\n",
    "# FEMR_LOGS_DIR = os.path.join(PATH_TO_OUTPUT_DIR, \"logs\")\n",
    "\n",
    "logger.info(\"...\")\n",
    "database = femr.datasets.PatientDatabase(FEMR_EXTRACT_DIR)\n",
    "logger.info(f\": {len(database)}\")\n",
    "logger.info(\"！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b211963b-5db4-4134-aa32-d435e4e6dddb",
   "metadata": {},
   "source": [
    "# Step 5: Vefify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65439165-31a0-4d03-b5ec-23391dd126b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "database = femr.datasets.PatientDatabase(FEMR_EXTRACT_DIR)\n",
    "all_patient_ids = list(database)\n",
    "patient_id: int = all_patient_ids[0]\n",
    "patient = database[patient_id]\n",
    "events = patient.events\n",
    "logger.info(f\"FEMR database saved to: {PATH_TO_OUTPUT_DIR}\")\n",
    "logger.info(f\"Num patients: {len(database)}\")\n",
    "logger.info(f\"Number of events in patient '{patient_id}': {len(events)}\")\n",
    "logger.info(f\"First event of patient '{patient_id}': {events[0]}\")\n",
    "logger.success(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602ee7a-167b-4a92-8234-2bdf3816896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(events)):\n",
    "    logger.info(f\"First event of patient '{patient_id}': {events[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d1a40-d44b-4220-82b4-66957809e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def human_bytes(n):\n",
    "    for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\",\"PB\"]:\n",
    "        if n < 1024: return f\"{n:.1f}{u}\"\n",
    "        n /= 1024\n",
    "    return f\"{n:.1f}EB\"\n",
    "\n",
    "def dir_overview(path):\n",
    "    p = Path(path)\n",
    "    print(\"Listing:\", p.resolve())\n",
    "    if not p.exists():\n",
    "        print(\"❌ \"); return\n",
    "    for item in sorted(p.iterdir()):\n",
    "        if item.is_dir():\n",
    "            files = [f for f in item.rglob(\"*\") if f.is_file()]\n",
    "            size = sum(f.stat().st_size for f in files)\n",
    "            print(f\"[DIR]  {item.name:30} files={len(files):6d}  size={human_bytes(size)}\")\n",
    "        else:\n",
    "            print(f\"[FILE] {item.name:30} {human_bytes(item.stat().st_size)}\")\n",
    "\n",
    "# dir_overview(FEMR_INPUT_DIR_CLEAN)\n",
    "# dir_overview(\"data/mimiciv_meds/data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a85a43-eb40-4205-bbb1-16959d1e60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_overview('/root/autodl-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b074c88-5b2a-474e-a380-29a9b561d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def human_bytes(n):\n",
    "    for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\",\"PB\"]:\n",
    "        if n < 1024: return f\"{n:.1f}{u}\"\n",
    "        n /= 1024\n",
    "    return f\"{n:.1f}EB\"\n",
    "\n",
    "def quick_csv_check(csv_path, sample_rows=5, required_cols=None):\n",
    "    path = Path(csv_path)\n",
    "    assert path.exists(), f\"：{path}\"\n",
    "    size = path.stat().st_size\n",
    "    print(f\"File: {path.resolve()}\\nSize: {human_bytes(size)}\")\n",
    "\n",
    "    raw = path.open(\"rb\").read(4096)\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(raw.decode(\"utf-8\", \"ignore\"))\n",
    "        delim = dialect.delimiter\n",
    "    except Exception:\n",
    "        delim = \",\"\n",
    "    print(\"Delimiter:\", repr(delim))\n",
    "\n",
    "    df_head = pd.read_csv(path, sep=delim, nrows=sample_rows, low_memory=False)\n",
    "    print(\"\\nColumns:\", list(df_head.columns))\n",
    "    print(f\"\\nHead ({sample_rows} rows):\")\n",
    "    print(df_head)\n",
    "\n",
    "    try:\n",
    "        df_probe = pd.read_csv(path, sep=delim, nrows=2000, low_memory=False)\n",
    "        print(\"\\nInferred dtypes (sample of 2k rows):\")\n",
    "        print(df_probe.dtypes)\n",
    "        print(\"\\nNA count (sample of 2k rows):\")\n",
    "        print(df_probe.isna().sum())\n",
    "    except Exception as e:\n",
    "        print(\"\\n(/) ：\", e)\n",
    "\n",
    "    if required_cols:\n",
    "        missing = [c for c in required_cols if c not in df_head.columns]\n",
    "        if missing:\n",
    "            print(\"❌ ：\", missing)\n",
    "        else:\n",
    "            print(\"✅ ：\", required_cols)\n",
    "\n",
    "    try:\n",
    "        with path.open(\"rb\") as f:\n",
    "            n_lines = sum(1 for _ in f)\n",
    "        print(f\"\\nLine count (): {n_lines}\")\n",
    "    except Exception as e:\n",
    "        print(\"\\n()：\", e)\n",
    "\n",
    "# quick_csv_check(Path(FEMR_INPUT_DIR_CLEAN) / \"a.csv\",\n",
    "#                 sample_rows=5,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a8d16-2624-447c-8c7e-6a7c97755c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_csv_check(Path(FEMR_INPUT_DIR_CLEAN) / \"a.csv\",\n",
    "                sample_rows=50,\n",
    "                required_cols=None)  # ， ['patient_id','time','concept_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8abe6-ed91-44ed-b150-bc824266d96d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d00e1-ba20-4d0c-bb82-dd82fbf4772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = Path(FEMR_INPUT_DIR_CLEAN) / \"a.csv\"\n",
    "out_path = Path(FEMR_INPUT_DIR_CLEAN) / \"a_small.csv\"\n",
    "patient_col = \"patient_id\"  # ← ，\n",
    "n_patients = 5  # ，\n",
    "\n",
    "uids = []\n",
    "seen = set()\n",
    "for chunk in pd.read_csv(csv_path, chunksize=200_000, dtype={patient_col: str}):\n",
    "    for uid in chunk[patient_col].astype(str).tolist():\n",
    "        if uid not in seen:\n",
    "            seen.add(uid)\n",
    "            uids.append(uid)\n",
    "            if len(uids) >= n_patients:\n",
    "                break\n",
    "    if len(uids) >= n_patients:\n",
    "        break\n",
    "\n",
    "print(\"Selected patients:\", uids)\n",
    "\n",
    "written_header = False\n",
    "with pd.read_csv(csv_path, chunksize=200_000, dtype={patient_col: str}) as reader:\n",
    "    for chunk in reader:\n",
    "        sub = chunk[chunk[patient_col].astype(str).isin(uids)]\n",
    "        if not sub.empty:\n",
    "            sub.to_csv(out_path, mode=\"a\", index=False, header=not written_header)\n",
    "            written_header = True\n",
    "\n",
    "print(\"Wrote:\", out_path, \"size:\", out_path.stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea8422-8752-45ea-b464-ec1a337b61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"---  3:  etl_simple_femr  ---\")\n",
    "\n",
    "# ##################################################################\n",
    "# ##################################################################\n",
    "\n",
    "EVENTS_CSV_PATH_t = os.path.join(FEMR_INPUT_DIR_CLEAN, \"a_small.csv\")\n",
    "FEMR_INPUT_DIR_CLEAN_t = os.path.join(PATH_TO_OUTPUT_DIR, \"femr_input_temp_test\")\n",
    "CLEAN_CSV_PATH_t = os.path.join(FEMR_INPUT_DIR_CLEAN_t, \"a_small.csv\")\n",
    "\n",
    "FEMR_EXTRACT_DIR = os.path.join(PATH_TO_OUTPUT_DIR, \"extract\")\n",
    "FEMR_LOGS_DIR = os.path.join(PATH_TO_OUTPUT_DIR, \"logs\")\n",
    "\n",
    "if os.path.exists(FEMR_EXTRACT_DIR): shutil.rmtree(FEMR_EXTRACT_DIR)\n",
    "if os.path.exists(FEMR_LOGS_DIR): shutil.rmtree(FEMR_LOGS_DIR)\n",
    "# if os.path.exists(FEMR_INPUT_DIR_CLEAN): shutil.rmtree(FEMR_INPUT_DIR_CLEAN)\n",
    "# os.makedirs(FEMR_INPUT_DIR_CLEAN)\n",
    "\n",
    "logger.info(f\" a_small.csv : {FEMR_INPUT_DIR_CLEAN_t}\")\n",
    "shutil.move(EVENTS_CSV_PATH_t, CLEAN_CSV_PATH_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ehr)",
   "language": "python",
   "name": "ehr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
