{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Evaluate: multi-label phenotyping (alternative notebook)\n",
    "\n",
    "## Purpose\n",
    "Alternative or dedicated training/evaluation path for the 25-label phenotyping task.\n",
    "\n",
    "## Inputs\n",
    "- Features + labels for phenotyping\n",
    "\n",
    "## Outputs\n",
    "- Metrics and result artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf95281-624b-4af5-8ed0-6639bba36d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, datetime, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, accuracy_score\n",
    "from loguru import logger\n",
    "\n",
    "from ehrshot.labelers.core import load_labeled_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61880b62-6ece-4ed0-908f-ffeef2463a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/root/autodl-tmp/femr\"\n",
    "\n",
    "TRAIN = {\n",
    "    \"db\":       f\"{ROOT}/train/extract\",\n",
    "    \"labels\":   f\"{ROOT}/train/femr_labels\",\n",
    "    \"features\": f\"{ROOT}/train/femr_features\",\n",
    "}\n",
    "TUNING = {\n",
    "    \"db\":       f\"{ROOT}/tuning/extract\",\n",
    "    \"labels\":   f\"{ROOT}/tuning/femr_labels\",\n",
    "    \"features\": f\"{ROOT}/tuning/femr_features\",\n",
    "}\n",
    "HELDOUT = {\n",
    "    \"db\":       f\"{ROOT}/held_out/extract\",\n",
    "    \"labels\":   f\"{ROOT}/held_out/femr_labels\",\n",
    "    \"features\": f\"{ROOT}/held_out/femr_features\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0471c-f949-4da5-9370-42bfcf45ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELING_FUNCTION = \"mimic_icu_phenotyping\"\n",
    "SHOT_STRAT = \"all\"  # \"few\"\n",
    "SHOTS_JSON = os.path.join(TRAIN[\"labels\"], LABELING_FUNCTION, f\"{SHOT_STRAT}_shots_data.json\")\n",
    "K_VALUES   = [-1]  # all ：[-1]；few ：[8,16,32]\n",
    "MODELS = [\"count\", \"clmbr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c54dc4-18da-4809-ae83-853443242458",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_FILES = {\n",
    "    \"train\": os.path.join(TRAIN[\"features\"], LABELING_FUNCTION, \"count_features.pkl\"),\n",
    "    \"tuning\": os.path.join(TUNING[\"features\"],LABELING_FUNCTION, \"count_features.pkl\"),\n",
    "    \"held\":   os.path.join(HELDOUT[\"features\"], LABELING_FUNCTION,\"count_features.pkl\"),\n",
    "}\n",
    "\n",
    "CLMBR_FILES = {\n",
    "    \"train\": os.path.join(TRAIN[\"features\"],LABELING_FUNCTION, \"clmbr_features.pkl\"),\n",
    "    \"tuning\":os.path.join(TUNING[\"features\"],LABELING_FUNCTION, \"clmbr_features.pkl\"),\n",
    "    \"held\":  os.path.join(HELDOUT[\"features\"], LABELING_FUNCTION, \"clmbr_features.pkl\"),\n",
    "}\n",
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555da059-0fab-4d06-98c2-2b180690d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, datetime, warnings\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "\n",
    "from loguru import logger\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "\n",
    "from ehrshot.labelers.core import load_labeled_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d577a6-2e18-42a5-9ee7-b6a3388c74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_minute_iso(x) -> str:\n",
    "    dt = pd.to_datetime(x).to_pydatetime()\n",
    "    return dt.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3479e17-003c-40fa-972e-1028427aaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_count_split_multilabel(path_pkl: str) -> Tuple[csr_matrix, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "     count_features.pkl，：(X_csr, patient_ids, label_values, label_times)\n",
    "     y； (X, rows_df[patient_id,time])\n",
    "    \"\"\"\n",
    "    with open(path_pkl, \"rb\") as f:\n",
    "        X, pids, _y_ignore, times = pickle.load(f)\n",
    "\n",
    "    if not issparse(X):\n",
    "        X = csr_matrix(np.asarray(X))\n",
    "    else:\n",
    "        X = X.tocsr()\n",
    "\n",
    "    rows_df = pd.DataFrame({\n",
    "        \"patient_id\": np.asarray(pids, dtype=int),\n",
    "        \"time\": [ _to_minute_iso(t) for t in times ],\n",
    "    })\n",
    "    assert X.shape[0] == len(rows_df), f\"X  {X.shape[0]}  rows {len(rows_df)} \"\n",
    "    return X, rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe8d78-294e-4d98-9ddc-ede82edd44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_present(d: dict, keys):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k], k\n",
    "    return None, None\n",
    "\n",
    "def load_clmbr_split(pkl_path: str) -> Tuple[Any, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "     (features, rows_df[patient_id,time])\n",
    "    -  dict: {\"features\"/\"representations\"/\"X\"/\"data_matrix\", \"patient_ids\"/\"pids\", \"time\"/\"labeling_time\"...}\n",
    "    - : (features, rows_df-like  (pid,time) )\n",
    "    \"\"\"\n",
    "    assert os.path.exists(pkl_path), f\" CLMBR : {pkl_path}\"\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        feats, _   = _first_present(obj, [\"features\", \"representations\", \"X\", \"data_matrix\"])\n",
    "        pids, _    = _first_present(obj, [\"patient_id\", \"patient_ids\", \"pids\"])\n",
    "        times, _   = _first_present(obj, [\"time\", \"times\", \"label_times\", \"labeling_time\"])\n",
    "        assert feats is not None and pids is not None and times is not None, f\"{pkl_path} : {list(obj.keys())}\"\n",
    "        rows_df = pd.DataFrame({\n",
    "            \"patient_id\": np.asarray(pids).astype(int),\n",
    "            \"time\": [ _to_minute_iso(t) for t in times ],\n",
    "        })\n",
    "        return feats, rows_df\n",
    "\n",
    "    if isinstance(obj, (list, tuple)) and len(obj) == 2:\n",
    "        feats, rows = obj\n",
    "        if isinstance(rows, pd.DataFrame):\n",
    "            rows_df = rows.copy()\n",
    "        else:\n",
    "            rows = np.asarray(rows)\n",
    "            assert rows.shape[1] == 2, f\" (patient_id,time)，: {rows.shape}\"\n",
    "            rows_df = pd.DataFrame(rows, columns=[\"patient_id\", \"time\"])\n",
    "        rows_df[\"patient_id\"] = rows_df[\"patient_id\"].astype(int)\n",
    "        rows_df[\"time\"] = rows_df[\"time\"].apply(_to_minute_iso)\n",
    "        return feats, rows_df\n",
    "\n",
    "    raise ValueError(f\" CLMBR pkl ：{type(obj)}\")\n",
    "\n",
    "\n",
    "def instances_from_labeled_csv_multilabel(path_to_labels_dir: str, task: str) -> List[Tuple[int, str, List[str]]]:\n",
    "    \"\"\"\n",
    "     <labels_dir>/<task>/labeled_patients.csv\n",
    "     [(patient_id, time_iso(), list[str])]\n",
    "    \"\"\"\n",
    "    lp_csv = os.path.join(path_to_labels_dir, task, \"labeled_patients.csv\")\n",
    "    lp = load_labeled_patients(lp_csv)\n",
    "    assert lp.get_labeler_type() == \"multilabel\", f\"{task}  multilabel （ {lp.get_labeler_type()}）\"\n",
    "    out, seen = [], set()\n",
    "    for pid, labels in lp.items():\n",
    "        for lab in labels:\n",
    "            t_iso = lab.time.replace(second=0, microsecond=0).isoformat(timespec=\"minutes\")\n",
    "            key = (int(pid), t_iso)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            v = lab.value\n",
    "            if v is None:\n",
    "                tags = []\n",
    "            elif isinstance(v, str):\n",
    "                tags = [v]\n",
    "            elif isinstance(v, (list, tuple, set)):\n",
    "                tags = list(v)\n",
    "            else:\n",
    "                tags = [str(v)]\n",
    "            out.append((int(pid), t_iso, [str(x) for x in tags]))\n",
    "    return out\n",
    "\n",
    "\n",
    "import ast\n",
    "def _ensure_list_of_str(x):\n",
    "    \"\"\"\n",
    "     list[str]：\n",
    "    - \"['A','B']\" -> [\"A\",\"B\"]\n",
    "    - [\"['A','B']\"] -> [\"A\",\"B\"]\n",
    "    - [\"A\",\"B\"] / {\"A\",\"B\"} / (\"A\",\"B\") -> [\"A\",\"B\"]\n",
    "    - \"A\" / 123 -> [\"A\"] / [\"123\"]\n",
    "    \"\"\"\n",
    "    if isinstance(x, (list, tuple, set)) and len(x) == 1:\n",
    "        only = next(iter(x))\n",
    "        if isinstance(only, str):\n",
    "            s = only.strip()\n",
    "            if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "                try:\n",
    "                    return [str(i) for i in ast.literal_eval(s)]\n",
    "                except Exception:\n",
    "                    return [t.strip().strip(\"'\\\"\") for t in s.strip(\"[]\").split(\",\") if t.strip()]\n",
    "        return [str(i) for i in x]\n",
    "\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                return [str(i) for i in ast.literal_eval(s)]\n",
    "            except Exception:\n",
    "                return [t.strip().strip(\"'\\\"\") for t in s.strip(\"[]\").split(\",\") if t.strip()]\n",
    "        return [s]\n",
    "\n",
    "    if isinstance(x, (list, tuple, set)):\n",
    "        return [str(i) for i in x]\n",
    "    return [str(x)]\n",
    "\n",
    "def _join_rows_multilabel(rows_df, samples, label_vocab):\n",
    "    lab2idx = {lab: i for i, lab in enumerate(label_vocab)}\n",
    "    key2row = {(int(pid), t): i for i,(pid,t) in enumerate(zip(rows_df[\"patient_id\"].values,\n",
    "                                                               rows_df[\"time\"].values))}\n",
    "    idxs, Y, missed, oov = [], [], 0, 0\n",
    "    for pid, t_iso, labs in samples:\n",
    "        r = key2row.get((int(pid), t_iso))\n",
    "        if r is None:\n",
    "            missed += 1; continue\n",
    "        labs = _ensure_list_of_str(labs)  # ★\n",
    "        y = np.zeros(len(label_vocab), dtype=int)\n",
    "        for lab in labs:\n",
    "            j = lab2idx.get(lab)\n",
    "            if j is None:\n",
    "                oov += 1; continue\n",
    "            y[j] = 1\n",
    "        idxs.append(r); Y.append(y)\n",
    "    if missed:\n",
    "        logger.warning(f\"[align] {missed}  rows_df ，\")\n",
    "    if oov:\n",
    "        logger.warning(f\"[align] {oov}  label_vocab，\")\n",
    "    return np.array(idxs, dtype=int), (np.vstack(Y) if Y else np.zeros((0, len(label_vocab)), dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f9863-809a-46de-b028-3e2d99982b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel(Y_true: np.ndarray, P: np.ndarray, thr: float = 0.5) -> Dict[str, float]:\n",
    "    out: Dict[str, float] = {}\n",
    "    # micro\n",
    "    try: out[\"auroc_micro\"] = float(roc_auc_score(Y_true.ravel(), P.ravel()))\n",
    "    except Exception: out[\"auroc_micro\"] = float(\"nan\")\n",
    "    try: out[\"auprc_micro\"] = float(average_precision_score(Y_true.ravel(), P.ravel()))\n",
    "    except Exception: out[\"auprc_micro\"] = float(\"nan\")\n",
    "    # macro\n",
    "    aucs, aprs = [], []\n",
    "    for j in range(Y_true.shape[1]):\n",
    "        yj, pj = Y_true[:, j], P[:, j]\n",
    "        if len(np.unique(yj)) < 2:  # 0/1\n",
    "            continue\n",
    "        try: aucs.append(roc_auc_score(yj, pj))\n",
    "        except Exception: pass\n",
    "        try: aprs.append(average_precision_score(yj, pj))\n",
    "        except Exception: pass\n",
    "    out[\"auroc_macro\"] = float(np.mean(aucs)) if aucs else float(\"nan\")\n",
    "    out[\"auprc_macro\"] = float(np.mean(aprs)) if aprs else float(\"nan\")\n",
    "    # F1\n",
    "    pred = (P >= thr).astype(int)\n",
    "    out[\"f1_micro\"] = float(f1_score(Y_true, pred, average=\"micro\", zero_division=0))\n",
    "    out[\"f1_macro\"] = float(f1_score(Y_true, pred, average=\"macro\", zero_division=0))\n",
    "    out[\"subset_acc\"] = float(accuracy_score(Y_true, pred))\n",
    "    return out\n",
    "\n",
    "def find_best_micro_threshold(Y_val: np.ndarray, P_val: np.ndarray,\n",
    "                              grid: np.ndarray | None = None) -> float:\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    best_thr, best_f1 = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        f1 = f1_score(Y_val, (P_val >= t).astype(int), average=\"micro\", zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, float(t)\n",
    "    return best_thr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b7123-7ff0-43af-a9c8-6f8bbca2a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _slice_rows(X, idxs: np.ndarray):\n",
    "    if isinstance(X, csr_matrix):\n",
    "        return X[idxs]\n",
    "    return np.asarray(X)[idxs]\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def build_multilabel_head(model_type: str, head: str, rep: int):\n",
    "    \"\"\"\n",
    "     (clf, needs_scaled_X)\n",
    "    - head='saga'  : OVR(LogReg SAGA) —— （，）\n",
    "    - head='sgd'   : OVR(SGD log_loss) —— ，\n",
    "    - head='nb'    : OVR(ComplementNB) —— （ count ，X 、）\n",
    "    \"\"\"\n",
    "    head = head.lower()\n",
    "    if head == \"saga\":\n",
    "        base = make_pipeline(\n",
    "            MaxAbsScaler(),\n",
    "            LogisticRegression(\n",
    "                solver=\"saga\", penalty=\"l2\",\n",
    "                C=0.5,\n",
    "                tol=1e-3,  # ，\n",
    "                max_iter=800,\n",
    "                n_jobs=-1,\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=rep,\n",
    "                verbose=0,\n",
    "            )\n",
    "        )\n",
    "        return OneVsRestClassifier(base, n_jobs=-1), True\n",
    "\n",
    "    if head == \"sgd\":\n",
    "        base = make_pipeline(\n",
    "            MaxAbsScaler() if model_type == \"count\" else StandardScaler(with_mean=False),\n",
    "            SGDClassifier(\n",
    "                loss=\"log_loss\", penalty=\"l2\",\n",
    "                alpha=1e-4,\n",
    "                max_iter=15,\n",
    "                early_stopping=True,\n",
    "                n_iter_no_change=3,\n",
    "                validation_fraction=0.1,\n",
    "                learning_rate=\"optimal\",\n",
    "                class_weight=None,  # ， sample_weight\n",
    "                random_state=rep,\n",
    "            )\n",
    "        )\n",
    "        return OneVsRestClassifier(base, n_jobs=-1), True\n",
    "\n",
    "    if head == \"nb\":\n",
    "        if model_type != \"count\":\n",
    "            raise ValueError(\"head='nb'  count 。\")\n",
    "        base = ComplementNB(alpha=0.1)\n",
    "        return OneVsRestClassifier(base, n_jobs=-1), False  # ， X\n",
    "\n",
    "    raise ValueError(f\" head: {head}\")\n",
    "\n",
    "    \n",
    "\n",
    "def run_one_model_multilabel(model_type: str,\n",
    "                             shots_for_k: Dict[int, dict],\n",
    "                             label_vocab: List[str],\n",
    "                             COUNT_FILES: Dict[str, str],\n",
    "                             CLMBR_FILES: Dict[str, str],\n",
    "                             HELDOUT_LABELS_DIR: str,\n",
    "                             TASK_NAME: str,\n",
    "                             use_pipeline_scaler: bool = True) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    shots_for_k: {rep: payload}；payload  list[str]  label_values_*_k\n",
    "    \"\"\"\n",
    "    logger.info(f\"=== [{model_type}] (multilabel)  ===\")\n",
    "    if model_type == \"count\":\n",
    "        X_tr, rows_tr = load_count_split_multilabel(COUNT_FILES[\"train\"])\n",
    "        X_va, rows_va = load_count_split_multilabel(COUNT_FILES[\"tuning\"])\n",
    "        X_te, rows_te = load_count_split_multilabel(COUNT_FILES[\"held\"])\n",
    "    else:\n",
    "        X_tr, rows_tr = load_clmbr_split(CLMBR_FILES[\"train\"])\n",
    "        X_va, rows_va = load_clmbr_split(CLMBR_FILES[\"tuning\"])\n",
    "        X_te, rows_te = load_clmbr_split(CLMBR_FILES[\"held\"])\n",
    "\n",
    "    held_instances = instances_from_labeled_csv_multilabel(HELDOUT_LABELS_DIR, TASK_NAME)\n",
    "    idx_te, Y_te = _join_rows_multilabel(rows_te, held_instances, label_vocab)\n",
    "    logger.info(f\"[{model_type}] held_out : {len(idx_te)}/{len(held_instances)}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for rep, payload in shots_for_k.items():\n",
    "        tr_samples = list(zip(payload[\"patient_ids_train_k\"],\n",
    "                              payload[\"label_times_train_k\"],\n",
    "                              payload[\"label_values_train_k\"]))   # list[str]\n",
    "        va_samples = list(zip(payload[\"patient_ids_val_k\"],\n",
    "                              payload[\"label_times_val_k\"],\n",
    "                              payload[\"label_values_val_k\"]))     # list[str]\n",
    "\n",
    "        idx_tr, Y_tr = _join_rows_multilabel(rows_tr, tr_samples, label_vocab)\n",
    "        idx_va, Y_va = _join_rows_multilabel(rows_va, va_samples, label_vocab)\n",
    "        logger.info(f\"[{model_type}|rep={rep}]  train={len(idx_tr)}/{len(tr_samples)}, val={len(idx_va)}/{len(va_samples)}\")\n",
    "\n",
    "        Xtr = _slice_rows(X_tr, idx_tr)\n",
    "        Xva = _slice_rows(X_va, idx_va)\n",
    "        Xte = _slice_rows(X_te, idx_te)\n",
    "\n",
    "        base_lr = make_pipeline(\n",
    "            MaxAbsScaler(),\n",
    "            LogisticRegression(\n",
    "                solver=\"saga\", penalty=\"l2\", C=1.0, tol=1e-6, max_iter=5000,\n",
    "                n_jobs=-1, class_weight=\"balanced\", random_state=rep, verbose=0\n",
    "            )\n",
    "        )\n",
    "        clf = OneVsRestClassifier(base_lr, n_jobs=-1)\n",
    "        clf.fit(Xtr, Y_tr)\n",
    "\n",
    "        P_tr = clf.predict_proba(Xtr)    # (n_tr, L)\n",
    "        P_va = clf.predict_proba(Xva)    # (n_va, L)\n",
    "        P_te = clf.predict_proba(Xte)    # (n_te, L)\n",
    "\n",
    "        best_thr = find_best_micro_threshold(Y_va, P_va)\n",
    "\n",
    "        m_tr = evaluate_multilabel(Y_tr, P_tr, thr=best_thr)\n",
    "        m_va = evaluate_multilabel(Y_va, P_va, thr=best_thr)\n",
    "        m_te = evaluate_multilabel(Y_te, P_te, thr=best_thr)\n",
    "\n",
    "        logger.success(\n",
    "            f\"[{model_type}|rep={rep}] \"\n",
    "            f\"AUROC micro/macro tr/va/te = \"\n",
    "            f\"{m_tr['auroc_micro']:.4f}/{m_tr['auroc_macro']:.4f} | \"\n",
    "            f\"{m_va['auroc_micro']:.4f}/{m_va['auroc_macro']:.4f} | \"\n",
    "            f\"{m_te['auroc_micro']:.4f}/{m_te['auroc_macro']:.4f} || \"\n",
    "            f\"F1 micro/macro tr/va/te @best = \"\n",
    "            f\"{m_tr['f1_micro']:.3f}/{m_tr['f1_macro']:.3f} | \"\n",
    "            f\"{m_va['f1_micro']:.3f}/{m_va['f1_macro']:.3f} | \"\n",
    "            f\"{m_te['f1_micro']:.3f}/{m_te['f1_macro']:.3f}\"\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"model\": model_type,\n",
    "            \"rep\": rep,\n",
    "            \"metrics_train\": m_tr,\n",
    "            \"metrics_val\": m_va,\n",
    "            \"metrics_test\": m_te,\n",
    "            \"thresholds\": {\"best_on_val_micro\": best_thr},\n",
    "            \"data_stats\": {\n",
    "                \"n_train\": int(Y_tr.shape[0]),\n",
    "                \"n_val\":   int(Y_va.shape[0]),\n",
    "                \"n_test\":  int(Y_te.shape[0]),\n",
    "                \"pos_rate_macro_train\": float(np.mean(Y_tr.mean(axis=0))) if Y_tr.size else float(\"nan\"),\n",
    "                \"pos_rate_macro_val\":   float(np.mean(Y_va.mean(axis=0))) if Y_va.size else float(\"nan\"),\n",
    "                \"pos_rate_macro_test\":  float(np.mean(Y_te.mean(axis=0))) if Y_te.size else float(\"nan\"),\n",
    "            },\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e551d-f0ec-4fae-92f9-18a6fc488106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shots(shots_json: str, task: str, ks: List[int]) -> Dict[int, Dict[int, dict]]:\n",
    "    with open(shots_json, \"r\") as f:\n",
    "        blob = json.load(f)\n",
    "    assert task in blob, f\"{shots_json}  {task}\"\n",
    "    task_dict = blob[task]\n",
    "    out: Dict[int, Dict[int, dict]] = {}\n",
    "    for k in ks:\n",
    "        if str(k) in task_dict:\n",
    "            out[k] = {int(rep): v for rep, v in task_dict[str(k)].items()}\n",
    "        elif k in task_dict:\n",
    "            out[k] = {int(rep): v for rep, v in task_dict[k].items()}\n",
    "        else:\n",
    "            logger.warning(f\"[shots] k={k}  {shots_json} ，\")\n",
    "    return out\n",
    "\n",
    "def _is_multilabel_task(path_to_labels_dir: str, task: str) -> bool:\n",
    "    lp_csv = os.path.join(path_to_labels_dir, task, \"labeled_patients.csv\")\n",
    "    lp = load_labeled_patients(lp_csv)\n",
    "    return lp.get_labeler_type() == \"multilabel\"\n",
    "\n",
    "def eval_one_model_multitask(model_name: str,\n",
    "                             SHOTS_JSON: str,\n",
    "                             LABELING_FUNCTION: str,\n",
    "                             K_VALUES: List[int],\n",
    "                             COUNT_FILES: Dict[str, str],\n",
    "                             CLMBR_FILES: Dict[str, str],\n",
    "                             TRAIN_LABELS_DIR: str,\n",
    "                             TUNING_LABELS_DIR: str,\n",
    "                             HELDOUT_LABELS_DIR: str,\n",
    "                             LABEL_VOCAB: List[str],\n",
    "                             out_dir: str = \"runs\"):\n",
    "    \"\"\"\n",
    "    ： multilabel，； binary （）。\n",
    "    \"\"\"\n",
    "    shots = load_shots(SHOTS_JSON, LABELING_FUNCTION, K_VALUES)\n",
    "    IS_MULTI = _is_multilabel_task(HELDOUT_LABELS_DIR, LABELING_FUNCTION)\n",
    "\n",
    "    all_results = []\n",
    "    for k, reps in shots.items():\n",
    "        logger.info(f\"******** K={k} | model={model_name} | task={'multilabel' if IS_MULTI else 'binary'} ********\")\n",
    "        if IS_MULTI:\n",
    "            res = run_one_model_multilabel(\n",
    "                model_type=model_name,\n",
    "                shots_for_k=reps,\n",
    "                label_vocab=LABEL_VOCAB,\n",
    "                COUNT_FILES=COUNT_FILES,\n",
    "                CLMBR_FILES=CLMBR_FILES,\n",
    "                HELDOUT_LABELS_DIR=HELDOUT_LABELS_DIR,\n",
    "                TASK_NAME=LABELING_FUNCTION,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\" multilabel； run_one_model。\")\n",
    "        for r in res:\n",
    "            r[\"k\"] = k\n",
    "            r[\"model\"] = model_name\n",
    "        all_results.extend(res)\n",
    "\n",
    "    rows = []\n",
    "    for r in all_results:\n",
    "        te = r[\"metrics_test\"]; va = r[\"metrics_val\"]\n",
    "        rows.append({\n",
    "            \"k\": r[\"k\"], \"model\": r[\"model\"], \"rep\": r[\"rep\"],\n",
    "            \"val_auroc_micro\": va[\"auroc_micro\"], \"val_auroc_macro\": va[\"auroc_macro\"],\n",
    "            \"val_auprc_micro\": va[\"auprc_micro\"], \"val_auprc_macro\": va[\"auprc_macro\"],\n",
    "            \"test_auroc_micro\": te[\"auroc_micro\"], \"test_auroc_macro\": te[\"auroc_macro\"],\n",
    "            \"test_auprc_micro\": te[\"auprc_micro\"], \"test_auprc_macro\": te[\"auprc_macro\"],\n",
    "            \"val_f1_micro\": va[\"f1_micro\"], \"val_f1_macro\": va[\"f1_macro\"],\n",
    "            \"test_f1_micro\": te[\"f1_micro\"], \"test_f1_macro\": te[\"f1_macro\"],\n",
    "            \"test_subset_acc\": te[\"subset_acc\"],\n",
    "            \"best_thr\": r[\"thresholds\"][\"best_on_val_micro\"],\n",
    "        })\n",
    "    summary = pd.DataFrame(rows).sort_values(by=[\"k\",\"model\",\"rep\"]).reset_index(drop=True)\n",
    "    print(f\"\\n===== Summary for model: {model_name} =====\")\n",
    "    print(summary)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    report_path = os.path.join(out_dir, f\"{model_name}-multilabel-summary.json\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump({\"summary\": rows, \"results\": all_results}, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[Saved] JSON report -> {report_path}\")\n",
    "\n",
    "    return summary, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeac4a4-ba8c-49e7-b2af-b2774108f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_vocab_from_json(path: str) -> list[str]:\n",
    "    import json\n",
    "    with open(path, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    if isinstance(d, dict) and \"root\" in d and isinstance(d[\"root\"], dict):\n",
    "        labels = list(d[\"root\"].keys())\n",
    "    elif isinstance(d, dict):\n",
    "        labels = list(d.keys())\n",
    "    elif isinstance(d, list):\n",
    "        labels = [str(x) for x in d]\n",
    "    else:\n",
    "        raise ValueError(f\" {path} ，={type(d)}\")\n",
    "\n",
    "    def _clean(s: str) -> str:\n",
    "        return \" \".join(str(s).split())\n",
    "\n",
    "    labels = [_clean(x) for x in labels]\n",
    "\n",
    "    assert len(labels) == 25, f\" 25 ， {len(labels)}； JSON \"\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c08ef1-3bc0-44ad-b9cb-406c960d6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELING_FUNCTION = \"mimic_icu_phenotyping\"\n",
    "PHENOTYPE_JSON_PATH = \"/root/autodl-tmp/phenotypes_ccs_from_parent.json\"  # 25 ，\n",
    "\n",
    "LABEL_VOCAB = load_label_vocab_from_json(PHENOTYPE_JSON_PATH)\n",
    "COUNT_FILES = {\n",
    "    \"train\": f\"/root/autodl-tmp/femr/train/femr_features/{LABELING_FUNCTION}/count_features.pkl\",\n",
    "    \"tuning\":f\"/root/autodl-tmp/femr/tuning/femr_features/{LABELING_FUNCTION}/count_features.pkl\",\n",
    "    \"held\":  f\"/root/autodl-tmp/femr/held_out/femr_features/{LABELING_FUNCTION}/count_features.pkl\",\n",
    "}\n",
    "CLMBR_FILES = {\n",
    "    \"train\": f\"/root/autodl-tmp/femr/train/femr_features/{LABELING_FUNCTION}/clmbr_features.pkl\",\n",
    "    \"tuning\":f\"/root/autodl-tmp/femr/tuning/femr_features/{LABELING_FUNCTION}/clmbr_features.pkl\",\n",
    "    \"held\":  f\"/root/autodl-tmp/femr/held_out/femr_features/{LABELING_FUNCTION}/clmbr_features.pkl\",\n",
    "}\n",
    "TRAIN_LABELS_DIR   = \"/root/autodl-tmp/femr/train/femr_labels\"\n",
    "TUNING_LABELS_DIR  = \"/root/autodl-tmp/femr/tuning/femr_labels\"\n",
    "HELDOUT_LABELS_DIR = \"/root/autodl-tmp/femr/held_out/femr_labels\"\n",
    "\n",
    "SHOTS_JSON = f\"/root/autodl-tmp/femr/train/femr_labels/{LABELING_FUNCTION}/all_shots_data.json\"  # few/all-shot JSON\n",
    "K_VALUES = [-1, 8, 16, 32, 64]  # all-shot\n",
    "\n",
    "summary, all_results = eval_one_model_multitask(\n",
    "    model_name=\"count\",  # \"clmbr\"\n",
    "    SHOTS_JSON=SHOTS_JSON,\n",
    "    LABELING_FUNCTION=LABELING_FUNCTION,\n",
    "    K_VALUES=K_VALUES,\n",
    "    COUNT_FILES=COUNT_FILES,\n",
    "    CLMBR_FILES=CLMBR_FILES,\n",
    "    TRAIN_LABELS_DIR=TRAIN_LABELS_DIR,\n",
    "    TUNING_LABELS_DIR=TUNING_LABELS_DIR,\n",
    "    HELDOUT_LABELS_DIR=HELDOUT_LABELS_DIR,\n",
    "    LABEL_VOCAB=LABEL_VOCAB,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ehr)",
   "language": "python",
   "name": "ehr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
