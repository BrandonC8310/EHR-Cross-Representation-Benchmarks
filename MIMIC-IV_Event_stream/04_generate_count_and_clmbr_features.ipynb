{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate features: COUNT and CLMBR\n\n## Purpose\nCompute COUNT features (Age + ontology-expanded CountFeaturizer) and CLMBR representations for each split and task.\n\n## Inputs\n- FEMR database at <BASE>/<split>/extract/\n- all_labels.csv at <BASE>/<split>/femr_labels/<TASK>/all_labels.csv\n- CLMBR binaries available in the active environment (clmbr_create_batches, clmbr_compute_representations)\n- CLMBR assets: model weights and dictionary\n\n## Outputs\n- COUNT features: <BASE>/<split>/femr_features/<TASK>/count_features.pkl\n- CLMBR features: <BASE>/<split>/femr_features/<TASK>/clmbr_features.pkl\n- CLMBR batches: <BASE>/<split>/femr_features/<TASK>/clmbr_batches/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4b55e-2e9f-43e0-8c86-bc62098ad71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import os, pickle\n",
    "from ehrshot.labelers.core import load_labeled_patients\n",
    "from femr.featurizers import AgeFeaturizer, CountFeaturizer, FeaturizerList\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464bf0c-447e-4435-8863-1fca091b0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/root/autodl-tmp/femr\" \n",
    "task = \"mimic_icu_phenotyping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff82316-ac05-4758-ab63-1b758cdb368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = {\n",
    "    \"train\":    {\"db\": f\"{BASE}/train/extract\",    \"labels\": f\"{BASE}/train/femr_labels\", \"features\": f\"{BASE}/train/femr_features\"},\n",
    "    \"tuning\":   {\"db\": f\"{BASE}/tuning/extract\",   \"labels\": f\"{BASE}/tuning/femr_labels\", \"features\": f\"{BASE}/tuning/femr_features\"},\n",
    "    \"held_out\": {\"db\": f\"{BASE}/held_out/extract\", \"labels\": f\"{BASE}/held_out/femr_labels\", \"features\": f\"{BASE}/held_out/femr_features\"},\n",
    "}\n",
    "\n",
    "NUM_THREADS = 15  # notebook ；\n",
    "FORCE = True\n",
    "\n",
    "def gen_count_features_one_split(db_dir, labels_dir, features_dir):\n",
    "    labels_path = os.path.join(labels_dir, \"all_labels.csv\")\n",
    "    out_path = os.path.join(features_dir, \"count_features.pkl\")\n",
    "    if (not FORCE) and os.path.exists(out_path):\n",
    "        logger.info(f\"[skip] {out_path} \")\n",
    "        return out_path\n",
    "\n",
    "    logger.info(f\"Loading labels: {labels_path}\")\n",
    "    labeled_patients = load_labeled_patients(labels_path)\n",
    "\n",
    "    age = AgeFeaturizer()\n",
    "    count = CountFeaturizer(is_ontology_expansion=True)\n",
    "    feats = FeaturizerList([age, count])\n",
    "\n",
    "    logger.info(\"Preprocess featurizers\")\n",
    "    feats.preprocess_featurizers(db_dir, labeled_patients, NUM_THREADS)\n",
    "\n",
    "    logger.info(\"Featurize patients\")\n",
    "    results = feats.featurize(db_dir, labeled_patients, NUM_THREADS)\n",
    "    feature_matrix, patient_ids, label_values, label_times = results\n",
    "\n",
    "    os.makedirs(features_dir, exist_ok=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    logger.success(f\"[ok] wrote {out_path}  \"\n",
    "                   f\"rows={feature_matrix.shape[0]}  cols={feature_matrix.shape[1]}\")\n",
    "    return out_path,feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229e7d0-e0a8-4f07-9622-6016e8b63e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=SPLITS['train']\n",
    "db_dir = cfg[\"db\"]\n",
    "labels_dir = cfg[\"labels\"]\n",
    "features_dir = cfg[\"features\"]\n",
    "task = \"mimic_icu_phenotyping\"\n",
    "labels_path = os.path.join(labels_dir,task,\"all_labels.csv\")\n",
    "out_path = os.path.join(features_dir,task,\"count_features.pkl\")\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "logger.info(f\"Loading labels: {labels_path}\")\n",
    "labeled_patients = load_labeled_patients(labels_path)\n",
    "\n",
    "age = AgeFeaturizer()\n",
    "count = CountFeaturizer(is_ontology_expansion=True)\n",
    "feats = FeaturizerList([age, count])\n",
    "\n",
    "logger.info(\"Preprocess featurizers\")\n",
    "feats.preprocess_featurizers(db_dir, labeled_patients, NUM_THREADS)\n",
    "\n",
    "logger.info(\"Featurize patients\")\n",
    "results = feats.featurize(db_dir, labeled_patients, NUM_THREADS)\n",
    "feature_matrix, patient_ids, label_values, label_times = results\n",
    "\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "with open(out_path, \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "logger.success(f\"[ok] wrote {out_path}  \"\n",
    "               f\"rows={feature_matrix.shape[0]}  cols={feature_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f513aa3-bc4c-42fe-b09d-68a7ddf3b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"=== Count features | {'train'} ===\")\n",
    "cfg=SPLITS['train']\n",
    "_, feats = gen_count_features_one_split(cfg[\"db\"], cfg[\"labels\"], cfg[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d315ef6-276c-4f04-8d57-266552637b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_count_features_one_split_rest(feats, task, db_dir, labels_dir, features_dir):\n",
    "    labels_path = os.path.join(labels_dir, task, \"all_labels.csv\")\n",
    "    out_path = os.path.join(features_dir, task, \"count_features.pkl\")\n",
    "    if (not FORCE) and os.path.exists(out_path):\n",
    "        logger.info(f\"[skip] {out_path} \")\n",
    "        return out_path\n",
    "\n",
    "    logger.info(f\"Loading labels: {labels_path}\")\n",
    "    labeled_patients = load_labeled_patients(labels_path)\n",
    "\n",
    "    logger.info(\"Featurize patients\")\n",
    "    results = feats.featurize(db_dir, labeled_patients, NUM_THREADS)\n",
    "    feature_matrix, patient_ids, label_values, label_times = results\n",
    "\n",
    "    os.makedirs(os.path.join(features_dir, task), exist_ok=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    logger.success(f\"[ok] wrote {out_path}  \"\n",
    "                   f\"rows={feature_matrix.shape[0]}  cols={feature_matrix.shape[1]}\")\n",
    "    return out_path,feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2dd38b-9d69-4848-ba1e-8e9caa7d9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"=== Count features | {'tuning'} ===\")\n",
    "cfg=SPLITS['tuning']\n",
    "gen_count_features_one_split_rest(feats, task, cfg[\"db\"], cfg[\"labels\"], cfg[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c6fe6-798b-4b0b-8993-99cd5a0a7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"=== Count features | {'held_out'} ===\")\n",
    "cfg=SPLITS['held_out']\n",
    "gen_count_features_one_split_rest(feats, task, cfg[\"db\"], cfg[\"labels\"], cfg[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f623947-5fc2-468d-83f0-6e7901cb86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FEATURE_DIR   = \"/root/autodl-tmp/femr/tuning/femr_features/mimic_icu_phenotyping\"\n",
    "COUNT_PKL    = os.path.join(FEATURE_DIR, \"count_features.pkl\")  # 4\n",
    "\n",
    "with open(COUNT_PKL, \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "X         = results[0]\n",
    "patient_ids   = np.asarray(results[1])\n",
    "label_values  = np.asarray(results[2])\n",
    "label_times   = np.asarray(results[3])\n",
    "\n",
    "print(\"X shape:\", X.shape)  # [, ]\n",
    "print(\"#patients:\", len(np.unique(patient_ids)))\n",
    "print(\"labels: pos =\", int(label_values.sum()), \"/\", len(label_values))\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(X[:5, :20].toarray()).head()\n",
    "pd.DataFrame(X[:5, :20].toarray()).style.format(\"{:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe9787-6041-4e34-a5fb-c1f65880dda8",
   "metadata": {},
   "source": [
    "# 5 clmbr features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8247e4-4cb9-4d9e-b965-b9ac37c7ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "python_executable_path = sys.executable\n",
    "path_to_bin = os.path.dirname(python_executable_path)\n",
    "path_to_clmbr_create_batches = os.path.join(path_to_bin, \"clmbr_create_batches\")\n",
    "\n",
    "if not os.path.exists(path_to_clmbr_create_batches):\n",
    "    raise FileNotFoundError(f\" {path_to_clmbr_create_batches}  'clmbr_create_batches'。\")\n",
    "    \n",
    "\n",
    "path_to_clmbr_compute_representations = os.path.join(path_to_bin, \"clmbr_compute_representations\")\n",
    "\n",
    "if not os.path.exists(path_to_clmbr_compute_representations):\n",
    "    raise FileNotFoundError(f\" {path_to_clmbr_compute_representations}  'clmbr_compute_representations'。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27da90-27b7-454c-9458-bec1b761215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "import subprocess, shlex\n",
    "\n",
    "MODELS_DIR = \"/root/code/MEDS_Process/ehrshot-benchmark/EHRSHOT_ASSETS/models\"  # clmbr/（）\n",
    "MODEL = \"clmbr\"  # \"motor\"\n",
    "FORCE = True\n",
    "\n",
    "def run(cmd: str):\n",
    "    logger.info(\"RUN: \" + cmd)\n",
    "    rc = subprocess.call(cmd, shell=True)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"Command failed (rc={rc}):\\n{cmd}\")\n",
    "\n",
    "def gen_clmbr_features_one_split(db_dir, labels_dir, features_dir, task):\n",
    "    model_dir = os.path.join(MODELS_DIR, MODEL)\n",
    "    path_clmbr_model = os.path.join(model_dir, \"clmbr_model\")\n",
    "    path_dictionary = \"/root/code/MEDS_Process/ehrshot-benchmark/EHRSHOT_ASSETS/models/clmbr/dictionary\"\n",
    "\n",
    "    assert os.path.exists(path_clmbr_model), f\"Missing model weights @ {path_clmbr_model}\"\n",
    "    assert os.path.exists(path_dictionary),  f\"Missing dictionary @ {path_dictionary}\"\n",
    "\n",
    "    labels_path = os.path.join(labels_dir, task, \"all_labels.csv\")\n",
    "    batches_dir = os.path.join(features_dir, task, f\"{MODEL}_batches\")\n",
    "    repr_path = os.path.join(features_dir, task, f\"{MODEL}_features.pkl\")\n",
    "\n",
    "    if FORCE:\n",
    "        os.system(f\"rm -rf {shlex.quote(batches_dir)}\")\n",
    "        os.system(f\"rm -rf {shlex.quote(repr_path)}\")\n",
    "\n",
    "    hier_flag = \"--is_hierarchical \" if MODEL == \"motor\" else \"\"\n",
    "    cmd_batches = (\n",
    "        f\"{path_to_clmbr_create_batches} {shlex.quote(batches_dir)}\"\n",
    "        f\" --data_path {shlex.quote(db_dir)}\"\n",
    "        f\" --dictionary {shlex.quote(path_dictionary)}\"\n",
    "        f\" --task labeled_patients\"\n",
    "        f\" --batch_size 131072\"\n",
    "        f\" --val_start 100\"\n",
    "        f\" --test_start 100\"\n",
    "        f\" {hier_flag}\"\n",
    "        f\" --labeled_patients_path {shlex.quote(labels_path)}\"\n",
    "    )\n",
    "    run(cmd_batches)\n",
    "\n",
    "    # 2) compute_representations\n",
    "    cmd_repr = (\n",
    "        f\"{path_to_clmbr_compute_representations} {shlex.quote(repr_path)}\"\n",
    "        f\" --data_path {shlex.quote(db_dir)}\"\n",
    "        f\" --batches_path {shlex.quote(batches_dir)}\"\n",
    "        f\" --model_dir {shlex.quote(path_clmbr_model)}\"\n",
    "    )\n",
    "    run(cmd_repr)\n",
    "    logger.success(f\"[ok] wrote {repr_path}\")\n",
    "\n",
    "for split, cfg in SPLITS.items():\n",
    "    logger.info(f\"=== CLMBR features | {split} ===\")\n",
    "    gen_clmbr_features_one_split(cfg[\"db\"], cfg[\"labels\"], cfg[\"features\"], task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06afad-e556-467e-b369-d65177ee3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pprint\n",
    "from femr.extension import dataloader\n",
    "\n",
    "DATA_PATH = \"/root/autodl-tmp/femr/train/extract\"\n",
    "BATCH_INFO = \"/root/autodl-tmp/femr/train/femr_features/mimic_icu_phenotyping/clmbr_batches/batch_info.msgpack\"\n",
    "\n",
    "loader = dataloader.BatchLoader(DATA_PATH, BATCH_INFO)\n",
    "\n",
    "print(\"num train/dev/test batches:\",\n",
    "      loader.get_number_of_batches(\"train\"),\n",
    "      loader.get_number_of_batches(\"dev\"),\n",
    "      loader.get_number_of_batches(\"test\"))\n",
    "\n",
    "batch = loader.get_batch(\"train\", 0)\n",
    "\n",
    "print(\"\\nTOP KEYS:\", list(batch.keys()))\n",
    "if \"task\" in batch:\n",
    "    print(\"TASK TYPE:\", type(batch[\"task\"]))\n",
    "    if isinstance(batch[\"task\"], dict):\n",
    "        print(\"TASK KEYS:\", list(batch[\"task\"].keys()))\n",
    "        for k in [\"labels\", \"label_values\", \"values\", \"y\", \"label_ages\"]:\n",
    "            if k in batch[\"task\"]:\n",
    "                v = batch[\"task\"][k]\n",
    "                try:\n",
    "                    shape = (len(v), len(v[0])) if hasattr(v, \"__getitem__\") else None\n",
    "                except Exception:\n",
    "                    shape = None\n",
    "                print(f\"task['{k}'] present; example type:\", type(v), \"example:\", (v[0] if len(v)>0 else None), \"shape:\", shape)\n",
    "    else:\n",
    "        print(\"TASK (non-dict):\", batch[\"task\"])\n",
    "\n",
    "print(\"\\nTRANSFORMER KEYS:\", list(batch[\"transformer\"].keys()))\n",
    "print(\"num_indices:\", batch.get(\"num_indices\"))\n",
    "print(\"patient_ids len:\", len(batch.get(\"patient_ids\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed7d00-c4fd-4623-a3c8-d80a3daa9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2af5a-d6cd-4864-9378-6f406bc1f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack, os\n",
    "\n",
    "def load_regular_map(dict_path):\n",
    "    with open(dict_path, \"rb\") as f:\n",
    "        unpacker = msgpack.Unpacker(f, raw=False)\n",
    "        regular = None\n",
    "        payloads = []\n",
    "        for obj in unpacker:\n",
    "            payloads.append(type(obj).__name__)\n",
    "            if isinstance(obj, dict) and (\"regular\" in obj or \"ontology_rollup\" in obj):\n",
    "                regular = obj.get(\"regular\")\n",
    "                meta = {k: type(v).__name__ for k, v in obj.items() if k != \"regular\"}\n",
    "                print(\"Found packed dict keys:\", list(obj.keys()))\n",
    "                print(\"Meta value types:\", meta)\n",
    "                break\n",
    "        print(\"Stream objects seen (types):\", payloads[:5], \"...\")\n",
    "        return regular\n",
    "\n",
    "REG = load_regular_map(\"/root/code/MEDS_Process/ehrshot-benchmark/EHRSHOT_ASSETS/models/clmbr/dictionary\")\n",
    "print(\"regular size:\", len(REG) if REG else None)\n",
    "some = list(REG.items())[:5]\n",
    "print(\"sample:\", some)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbdb8c-1b64-4890-bcb8-61242ee488c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, msgpack\n",
    "\n",
    "ASSETS_DIR = \"/root/code/MEDS_Process/ehrshot-benchmark/EHRSHOT_ASSETS/models/clmbr\"\n",
    "PATH_T2C   = os.path.join(ASSETS_DIR, \"token_2_code.json\")\n",
    "OUT_DICT   = os.path.join(ASSETS_DIR, \"dictionary.vocab.msgpack\")\n",
    "\n",
    "AGE_STATS = None  # : {\"mean\": 45.3, \"std\": 18.7}\n",
    "\n",
    "def _load_any_jsons(path: str):\n",
    "    \"\"\" JSON （ JSON /  JSON / JSONL ）。\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    objs = []\n",
    "\n",
    "    try:\n",
    "        objs.append(json.loads(data.decode(\"utf-8-sig\")))\n",
    "        return objs\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for line in data.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                objs.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "        if objs:\n",
    "            return objs\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    s = data.decode(\"utf-8\", errors=\"ignore\")\n",
    "    dec = json.JSONDecoder()\n",
    "    idx = 0\n",
    "    N = len(s)\n",
    "    while idx < N:\n",
    "        while idx < N and s[idx].isspace():\n",
    "            idx += 1\n",
    "        if idx >= N:\n",
    "            break\n",
    "        try:\n",
    "            obj, end = dec.raw_decode(s, idx)\n",
    "            objs.append(obj)\n",
    "            idx = end\n",
    "        except Exception:\n",
    "            idx += 1\n",
    "    if not objs:\n",
    "        raise ValueError(\" JSON \")\n",
    "    return objs\n",
    "\n",
    "def build_dictionary_from_token2code(path_t2c: str, out_path: str, age_stats=None):\n",
    "    \"\"\" token_2_code（ JSON、、 JSONL） CLMBR  msgpack。\"\"\"\n",
    "    objs = _load_any_jsons(path_t2c)\n",
    "\n",
    "    code2token = {}\n",
    "    for obj in objs:\n",
    "        if isinstance(obj, dict):\n",
    "            keys = list(obj.keys())\n",
    "            is_a = keys and all(isinstance(k, str) and k.isdigit() for k in keys[: min(5, len(keys))])\n",
    "            if is_a:\n",
    "                for tok_str, code in obj.items():\n",
    "                    if not isinstance(code, str):\n",
    "                        raise ValueError(\" A  value  code_string(str)\")\n",
    "                    code2token[code] = int(tok_str)\n",
    "            else:\n",
    "                for code, tok in obj.items():\n",
    "                    if not isinstance(tok, int):\n",
    "                        raise ValueError(\" B  value  token_id(int)\")\n",
    "                    code2token[code] = tok\n",
    "        elif isinstance(obj, list):\n",
    "            for it in obj:\n",
    "                if isinstance(it, (list, tuple)) and len(it) == 2:\n",
    "                    tok, code = it\n",
    "                    code2token[str(code)] = int(tok) if not isinstance(tok, int) else tok\n",
    "                elif isinstance(it, dict):\n",
    "                    if \"token\" in it and \"code\" in it:\n",
    "                        tok, code = it[\"token\"], it[\"code\"]\n",
    "                        code2token[str(code)] = int(tok) if not isinstance(tok, int) else tok\n",
    "\n",
    "    if not code2token:\n",
    "        raise ValueError(\" code->token_id ， token_2_code.json \")\n",
    "\n",
    "    dictionary_obj = {\"regular\": code2token}\n",
    "    if age_stats is not None:\n",
    "        dictionary_obj[\"age_stats\"] = {\"mean\": float(age_stats[\"mean\"]), \"std\": float(age_stats[\"std\"])}\n",
    "\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        msgpack.pack(dictionary_obj, f, use_bin_type=True)\n",
    "\n",
    "    print(f\"[ok] wrote dictionary: {out_path}\")\n",
    "    print(\"vocab size =\", len(code2token),\n",
    "          \"| min_id =\", min(code2token.values()),\n",
    "          \"| max_id =\", max(code2token.values()))\n",
    "    return out_path\n",
    "\n",
    "build_dictionary_from_token2code(PATH_T2C, OUT_DICT, age_stats=AGE_STATS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ehr)",
   "language": "python",
   "name": "ehr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}